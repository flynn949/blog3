summary(regression_model)
beta_0 <- 58 #This is the mean mark in second session for untutored students
beta_1 <- 0.9
beta_2 <- 3
sigma <- 2 #Luck in second session.
students$second_session_marks.mu <- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored
students$second_session_marks <- rnorm(n_students, students$second_session_marks.mu, sigma)
#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:
students$second_session_marks[students$second_session_marks > 100] <- 100
students$second_session_marks[students$second_session_marks < 0] <- 0
hist(students$second_session_marks, breaks = 15)
plot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))
regression_model <- lm(second_session_marks ~ 1 + first_session_marks + tutored, data = students)
summary(regression_model)
meanmark - beta_0
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
n_students <- 500
n_tutoring <- 50
meanmark <- 60.0
marks_sd <- 12.0
aptitude <- rnorm(n_students, 0.0, 1.0)
students <- data.frame(aptitude)
luck <- 2
students$first_session_marks <- rnorm(n_students, students$aptitude*marks_sd + meanmark, luck)
#It is possible for soem values to fall off the scale.
students$first_session_marks[students$first_session_marks > 100] <- 100
students$first_session_marks[students$first_session_marks < 0] <- 0
hist(students$first_session_marks, breaks = 15)
students <- students[order(students$first_session_marks),]
n_tutoring_places_accepted <- 0
tutored <- rep(0, n_students)
i <- 1
while (n_tutoring_places_accepted < n_tutoring & i < n_students) {
rval <- runif(1)
if (rval < 0.8 - students$first_session_marks[i]/100){
tutored[[i]] <- 1
n_tutoring_places_accepted <- n_tutoring_places_accepted + 1
}
i <- i + 1
}
students$tutored <- tutored
sum(tutored)
beta_0 <- 58 #This is the mean mark in second session for untutored students
beta_1 <- 0.9
beta_2 <- 3
sigma <- 2 #Luck in second session.
students$second_session_marks.mu <- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored
students$second_session_marks <- rnorm(n_students, students$second_session_marks.mu, sigma)
#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:
students$second_session_marks[students$second_session_marks > 100] <- 100
students$second_session_marks[students$second_session_marks < 0] <- 0
hist(students$second_session_marks, breaks = 15)
plot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))
regression_model <- lm(second_session_marks ~ 1 + first_session_marks + tutored, data = students)
summary(regression_model)
beta_0 <- 58 #This is the mean mark in second session for untutored students
beta_1 <- 0.9
beta_2 <- 7
sigma <- 2 #Luck in second session.
students$second_session_marks.mu <- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored
students$second_session_marks <- rnorm(n_students, students$second_session_marks.mu, sigma)
#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:
students$second_session_marks[students$second_session_marks > 100] <- 100
students$second_session_marks[students$second_session_marks < 0] <- 0
hist(students$second_session_marks, breaks = 15)
plot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))
regression_model <- lm(second_session_marks ~ 1 + first_session_marks + tutored, data = students)
summary(regression_model)
beta_0 <- 58 #This is the mean mark in second session for untutored students
beta_1 <- 0.9
beta_2 <- 3
sigma <- 2 #Luck in second session.
students$second_session_marks.mu <- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored
students$second_session_marks <- rnorm(n_students, students$second_session_marks.mu, sigma)
#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:
students$second_session_marks[students$second_session_marks > 100] <- 100
students$second_session_marks[students$second_session_marks < 0] <- 0
hist(students$second_session_marks, breaks = 15)
plot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))
regression_model <- lm(second_session_marks ~ 1 + first_session_marks + tutored, data = students)
summary(regression_model)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 2, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
aptitude_markscale <- meanmark + aptitude*marks_sd
first_session <- rnorm(nstudents, aptitude_markscale, 2)
second_session <- beta0 + beta1*aptitude_markscale + rnorm(nstudents, 0, sigma_secondsession)
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
second_session[tutored.indices] <- second_session[tutored.indices] + beta3
lmod <- summary(lm(second_session ~ first_session + tutored))
return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 5))
mean(pvals <= 0.05)
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
n_students <- 500
n_tutoring <- 50
meanmark <- 60.0
marks_sd <- 12.0
aptitude <- rnorm(n_students, 0.0, 1.0)
students <- data.frame(aptitude)
luck <- 2
students$first_session_marks <- rnorm(n_students, students$aptitude*marks_sd + meanmark, luck)
#It is possible for soem values to fall off the scale.
students$first_session_marks[students$first_session_marks > 100] <- 100
students$first_session_marks[students$first_session_marks < 0] <- 0
hist(students$first_session_marks, breaks = 15)
students <- students[order(students$first_session_marks),]
n_tutoring_places_accepted <- 0
tutored <- rep(0, n_students)
i <- 1
while (n_tutoring_places_accepted < n_tutoring & i < n_students) {
rval <- runif(1)
if (rval < 0.8 - students$first_session_marks[i]/100){
tutored[[i]] <- 1
n_tutoring_places_accepted <- n_tutoring_places_accepted + 1
}
i <- i + 1
}
students$tutored <- tutored
sum(tutored)
beta_0 <- 58 #This is the mean mark in second session for untutored students
beta_1 <- 0.9
beta_2 <- 3
sigma <- 2 #Luck in second session.
students$second_session_marks.mu <- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored
students$second_session_marks <- rnorm(n_students, students$second_session_marks.mu, sigma)
#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:
students$second_session_marks[students$second_session_marks > 100] <- 100
students$second_session_marks[students$second_session_marks < 0] <- 0
hist(students$second_session_marks, breaks = 15)
plot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))
regression_model <- lm(second_session_marks ~ 1 + first_session_marks + tutored, data = students)
summary(regression_model)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 58, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
aptitude_markscale <- meanmark + aptitude*marks_sd
first_session <- rnorm(nstudents, aptitude_markscale, 2)
second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
second_session <- second_session[sort(order(second_session)[first_session])]
second_session[tutored.indices] <- second_session[tutored.indices] + beta3
lmod <- summary(lm(second_session ~ first_session + tutored))
return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2))
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2)
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2))
order(c(5,8,3)[c(3,6,1)])
c(5,8,3)[order(c(5,8,3)[c(3,6,1)])]
c(5,8,3)[order(c(6,7,1)]
c(5,8,3)[order(c(6,7,1))]
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 58, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
aptitude_markscale <- meanmark + aptitude*marks_sd
students$first_session_marks <- rnorm(nstudents, students$aptitude*12 + meanmark, 2)
students$second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
students <- students[order(students$first_session_marks),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- students$second_session + beta3*students$tutored
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2))
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 58, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
aptitude_markscale <- meanmark + aptitude*marks_sd
students$first_session <- rnorm(nstudents, students$aptitude*12 + meanmark, 2)
students$second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- students$second_session + beta3*students$tutored
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2))
mean(pvals <= 0.05)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 58, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
aptitude_markscale <- meanmark + aptitude*marks_sd
students$first_session <- rnorm(nstudents, students$aptitude*12 + 60, 2)
students$second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- students$second_session + beta3*students$tutored
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}
pvals <- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 2))
mean(pvals <= 0.05)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
aptitude_markscale <- meanmark + aptitude*marks_sd
students$first_session <- rnorm(nstudents, students$aptitude*12 + 60, 2)
students$second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- students$second_session + tutoring_effect*students$tutored
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.
}
pvals <- replicate(1000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 2.4, sigma_secondsession = 2))
mean(pvals <= 0.05)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9, marks_sd = 12){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
students$first_session <- rnorm(nstudents, students$aptitude*marks_sd + 60, 2)
students$second_session <- beta0 + beta1*aptitude*marks_sd + rnorm(nstudents, 0, sigma_secondsession)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- students$second_session + tutoring_effect*students$tutored
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.
}
pvals <- replicate(1000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 2.4, sigma_secondsession = 2))
mean(pvals <= 0.05)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9, marks_sd = 12){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
students$first_session <- rnorm(nstudents, students$aptitude*marks_sd + 60, 2)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- rnorm(nstudents, beta0 + beta1*aptitude*marks_sd + tutoring_effect*students$tutored, sigma_secondsession)
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.
}
pvals <- replicate(1000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 2.4, sigma_secondsession = 2))
mean(pvals <= 0.05)
power_simulation <- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9, marks_sd = 12){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
students$first_session <- rnorm(nstudents, students$aptitude*marks_sd + 60, 2)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- rnorm(nstudents, beta0 + beta1*aptitude*marks_sd + tutoring_effect*students$tutored, sigma_secondsession)
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(students)
#return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.
}
power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 2.4, sigma_secondsession = 2)
test <- power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 2.4, sigma_secondsession = 2)
View(test)
invlogit <- function(x){1/(1+exp(-x))}
logit <- function(x){log(x/(1-x))}
power_simulation <- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9, marks_sd = 12){
nstudents <- n_tutored + n_untutored
aptitude <- sort(rnorm(nstudents, 0, 1))
students <- data.frame(aptitude)
students$first_session <- rnorm(nstudents, students$aptitude*marks_sd + 60, 2)
students <- students[order(students$first_session),]
tutored.indices <- sample(seq(1,n_tutored*2), n_tutored)
tutored <- rep(0, nstudents)
tutored[tutored.indices] <- 1
students$tutored <- tutored
students$second_session <- rnorm(nstudents, beta0 + beta1*aptitude*marks_sd + tutoring_effect*students$tutored, sigma_secondsession)
lmod <- summary(lm(second_session ~ first_session + tutored, data = students))
return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.
}
pvals <- replicate(1000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 1, sigma_secondsession = 2))
mean(pvals <= 0.05)
library(blogdown)
serve_site()
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(kableExtra)
library(rstan)
set.seed(5646)
gaussian <- function(height,centre,width,x){
return(height * exp(-0.5* ( ((x - centre)^2)/(width^2)) ))
}
invlogit <- function(x){exp(x)/(1+exp(x))}
exampleconc <- seq(60,80,by = 0.5)
exampleresp_prob <- gaussian(0.5, 70, 1.5, exampleconc)
n_trials <- rbinom(length(exampleconc), size = 100, prob = 0.8)
exampleresp_resp <- rbinom(length(exampleconc), n_trials, prob = exampleresp_prob)
exampleconc2 <- seq(60,80,by = 1)
n_trials2 <- rbinom(length(exampleconc2), size = 16, prob = 0.8)
exampleresp_prob2 <- gaussian(0.4, 65, 1.5, exampleconc2)
exampleresp_resp2 <- rbinom(length(exampleconc2), n_trials2, prob = exampleresp_prob2)
par( mfrow = c(1,2))
plot(exampleconc, exampleresp_resp, xlab = "Concentration", ylab = "Successes", main = "Mutant A (many trials)")
plot(exampleconc2, exampleresp_resp2, xlab = "Concentration", ylab = "Successes", main = "Mutant B (fewer trials)")
# Give each mutant a name.
n_mutants <- 6
mutants <- LETTERS[1:n_mutants]
# Population-level intercepts
pop_height_logodds <- 0
pop_centre <- 70
pop_width <- 1.5
#Population scaling parameters
group_scale_height_logodds <- 1
group_scale_centre <- 4
group_scale_width <- 0.5
#invlogit function for converting log odds to probability
invlogit <- function(x){exp(x)/(1+exp(x))}
#set random seed for reproducibility
set.seed(100)
height_raw_logodds <- rnorm(n = n_mutants, mean = 0, sd = 1)
centre_raw <- rnorm(n = n_mutants, mean = 0, sd = 1)
width_raw <- rnorm(n = n_mutants, mean = 0, sd = 1)
mutant_height_logodds <- pop_height_logodds + group_scale_height_logodds * height_raw_logodds
mutant_centre <- pop_centre + group_scale_centre * centre_raw
mutant_width <- pop_width + group_scale_width * width_raw
# Transform these parameters from log-odds to probability
mutant_height_p <- invlogit(mutant_height_logodds)
# Data summary
mutant.frame <- data.frame("mutant" = mutants,
"height_logodds" = mutant_height_logodds,
"height_p" = mutant_height_p,
"centre" = mutant_centre,
"width" = mutant_width)
kable(mutant.frame, format = "html", digits = 1)%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
# Predictor variable
conc_range <- 10
conc_interval <- 1
conc <- seq(pop_centre-conc_range, pop_centre + conc_range, conc_interval)
# Maximum number of repeats/trials
max_trials = 10
#Probability of max trials
prob_trials = 0.95 #A high value to simulate a situation where there is an intended number of trials, but some have failed for technical reasons.
gaussian <- function(height,centre,width,x){
return(height * exp(-0.5* ( ((x - centre)^2)/(width^2)) ))
}
probability.matrix <- matrix(data = NA,
nrow = length(conc),
ncol = n_mutants,
dimnames = list(conc, mutants))
for (j in 1:n_mutants){
for (i in 1:length(conc)){
probability.matrix[i,j] <- gaussian(mutant_height_p[j],
mutant_centre[j],
mutant_width[j],
conc[i])
}
}
n_trials.matrix <- matrix(data = rbinom(n = length(conc)*n_mutants,
size = max_trials,
prob = prob_trials),
nrow = length(conc),
ncol = n_mutants,
dimnames = list(conc, mutants))
# Simulate experiments - get the number of successes given the probability from the above matrix and the number of trials.
successes.matrix <- matrix(data = NA,
nrow = length(conc),
ncol = n_mutants,
dimnames = list(conc, mutants))
for (j in 1:n_mutants){
for (i in 1:length(conc)){
successes.matrix[i,j] <- rbinom(1,
prob =  probability.matrix[i,j],
size = n_trials.matrix[i,j])
}
}
# Use the melt function from reshape2, this is similar to tidyr's gather, but it also works on matrices and arrays.
# This converts data from wide to long format.
n_trials.frame <- gather(as.data.frame(n_trials.matrix)))
# Use the melt function from reshape2, this is similar to tidyr's gather, but it also works on matrices and arrays.
# This converts data from wide to long format.
n_trials.frame <- gather(as.data.frame(n_trials.matrix))
View(n_trials.frame)
thing <- as.data.frame(n_trials.matrix)
View(thing)
thingy <- gather(thing)
View(thingy)
# Use the melt function from reshape2, this is similar to tidyr's gather, but it also works on matrices and arrays.
# This converts data from wide to long format.
n_trials.frame <- melt(n_trials.matrix)
library(reshape2)
# Use the melt function from reshape2, this is similar to tidyr's gather, but it also works on matrices and arrays.
# This converts data from wide to long format.
n_trials.frame <- melt(n_trials.matrix)
View(n_trials.frame)
View(thingy)
View(n_trials.frame)
View(n_trials.frame)
View(thingy)
View(n_trials.frame)
View(thingy)
View(n_trials.frame)
View(thingy)
View(thing)
knitr::opts_chunk$set(echo = TRUE)
true_vals <- sample(c(0,1), nvals, prob = c(0.7,0.3), replace = TRUE)
nvals <- 100000
true_vals <- sample(c(0,1), nvals, prob = c(0.7,0.3), replace = TRUE)
1 - 0
1 - 1
guessf1 <- function(class1prob = 0.3){
nvals <- 100000
class0prob = 1 - class1prob
true_vals <- sample(c(0,1), nvals, prob = c(class0prob,class1prob), replace = TRUE)
guesses <-sample(c(0,1), nvals, prob = c(class0prob,class1prob), replace = TRUE)
precision <- precisionfunction(true_vals, guesses)
recall <- recallfunction(true_vals, guesses)
f1 <- f1function(precision, recall)
return(f1)
}
f1vals <- replicate(100, guessf1(0.3))
#Here, x is the true value, and y is the predicted value
precisionfunction <- function(x, y, class = 1){sum(y == x & x == class)/(sum(y == x & x == class) + sum(y == class & x == (1 - class))}
#Here, x is the true value, and y is the predicted value
precisionfunction <- function(x, y, class = 1){sum(y == x & x == class)/((sum(y == x & x == class) + sum(y == class & x == (1 - class))))}
recallfunction <- function(x, y, class = 1){sum(y == x & x == class)/((sum(y == x & x == class) + sum(y ==  (1 - class) & x == class)))}
f1function <- function(prec, rec){2*prec*rec/(prec+rec)}
guessf1 <- function(class1prob = 0.3){
nvals <- 100000
class0prob = 1 - class1prob
true_vals <- sample(c(0,1), nvals, prob = c(class0prob,class1prob), replace = TRUE)
guesses <-sample(c(0,1), nvals, prob = c(class0prob,class1prob), replace = TRUE)
precision <- precisionfunction(true_vals, guesses)
recall <- recallfunction(true_vals, guesses)
f1 <- f1function(precision, recall)
return(f1)
}
f1vals <- replicate(100, guessf1(0.3))
mean(f1vals)
classintervals <- seq(0.1, 0.9, by = 0.1)
f1vals <- c()
for (i in 1:length(classintervals)){
f1vals[[i]] <- guessf1(classintervals[i])
}
plot(classintervals, f1vals)
classintervals <- seq(0.1, 0.9, by = 0.1)
f1vals <- c()
for (i in 1:length(classintervals)){
f1vals[[i]] <- guessf1(classintervals[i])
}
plot(classintervals, f1vals)
lm(f1vals ~ classintervals)
library(readr)
pst <- read_csv("S:/Learning Analytics/Research/Sprints/PipelinePredictiveModel/Pipeline2/predictions.studentlevel.test.csv")
View(pst)
sum(pst$OUTCOME)/nrow(pst)
pst2 <- pst %>% filter(COURSE_FACULTY == "Faculty of Business")
sum(pst2$OUTCOME)/nrow(pst)
View(pst2)
pst <- group_by(COURSE_FACULTY) %>% summarise(fr = sum(OUTCOME)/n())
pst3 <- pst %>% group_by(COURSE_FACULTY) %>% summarise(fr = sum(OUTCOME)/n())
View(pst3)
serve_site()
library(blogdown)
serve_site()
library(blogdown)
blogdown:::new_post_addin()
serve_site()
