---
title: Multilevel logistic regression (again)
author: Flynn Hill
date: '2018-11-26'
output: blogdown::html_page
slug: multilevel-logistic-regression
categories: []
tags:
  - multilevel modelling
  - logistic regression
  - variance
header:
  caption: ''
  image: ''
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(lme4)
library(tidyverse)
```


I recently attended a whole-day workshop on multilevel modelling organised by [NIASRA](https://niasra.uow.edu.au/index.html) and taught by Professor [Mark Tranmer](https://www.gla.ac.uk/schools/socialpolitical/staff/marktranmer/), who is an expert in the field from the University of Glasgow. 


One part of the workshop that I realised I didn't have any familiarity with was the idea of how variance works in logistic regression, and in particular in a multilevel logistic regression.

Here is one way of looking at logistic regression, in a formulation that reflects how a glm works:

$$ y_{prob} = \text{logit}^{-1}(X\beta) \\ y_{obs} \sim \text{Binomial}(y_{prob}) $$

Where $\text{logit}^{-1}$ is the inverse logit function: $exp(X\beta)/(exp(X\beta) + 1)$ which transforms the linear predictor $X\beta$ from log-odds to probabilities $y_{prob}$. These then give the observed outcomes $y_{obs}$ according to a binomial distribution.

In multilevel logistic regression, it turns out that adding group (random) effects increases the residual variance. This presents a challenge when we are trying to decide whether it is worth including a group effect. What can we say about how much of the total variance is explained by the group effect?

To explore this issue, the latent variable model is helpful. Although mathematically equivalent to the glm formulation above, this conceptualisation offers an easier way to look at the variance.

In the latent variable model, there is an underlying linear variable $y^*$, which produces a $y_{obs}$ value of 1 when it is above 0 and 0 when it is below 0. $y^*$ is given by a predictor with standard logistic distribution, centred at a value given by a linear predictor, with a scale of 1. This distribution has a variance of (pi^2)/3 = 3.29.


$$ y_{prob}=X\beta \\ y^* \sim \text{logistic}(y_{prob}, 1) \\ y_{obs}=y^*\geq0\;?\; 1 : 0 $$


The logistic distribution has a cumulative distribution function given by 

$$ \frac{1}{2} + \frac{1}{2}\text{tanh}(\frac{x-\mu}{2s})$$

The CDF of a logistic distribution (black) with scale parameter $s=1$ is almost identical to a normal distribution with a variance of 3.29 (red).


```{r}

x <- seq(-10,10,0.01)
pl <- plogis(x, location = 0, scale = 1)
nl<- pnorm(x, mean = 0, sd = sqrt(pi*pi/3))
plot(x,pl,t="l")
lines(x, nl, col="red")

```

How does this apply to multilevel models?  We can put all population effects into $y_{prob}$ and calculate $y^*$ from a logistic distribution centred on $y_{prob}$, plus an additional intercept term $u_j$ for the group effect, which stretches out $y^*$:

$$ y_{prob}=X\beta \\ y^* \sim \text{logistic}(y_{prob}, 1)+u_j \\ y_{obs}=y^*\geq0\;?\; 1 : 0 $$

Thus, the population effects are set to contribute a variance of 3.29, while any group effects are added on top of this variance. If we subtract 3.29 from the total residual variance, we can find the variance attributable to the group effect, and take this as a proportion of the total variance.


## Simulation

```{r}
set.seed(1)
```


Here, I simulate a dataset by way of the latent variable formulation above. This allows us to know the true proportion of variance explained by the group effect.

Setup:

*  Binary outcome yobs
*  One continuous predictor x1
*  One binary predictor x2
*  A group level effect with varying intercepts uj.

To make it interesting, the distribution of the binary predictor x2 will vary by group, with some groups having many more observations with x2=1 and some having very few.


```{r}

multibin <- function(n, J, beta1, beta2, jstd){
  group <- sample(J, n, replace=TRUE) #group membership indicator
  x1 <- rnorm(n,0,1) #standard continuous predictor variable
  x2 <- rep(NA,length(group))
  group_probs <- runif(20) #probability of x2=1 for members of each group
  for (i in 1:length(group)){
    x2[i] <- sample(c(0,1), prob = c(1 - group_probs[group[i]], group_probs[group[i]]))
  }
  uj <- rnorm(J,0,jstd) #Group intercepts, normally distributed with standard deviation jstd
  yprob <- rep(NA,n)
  for (i in 1:n){
    yprob[i] <- x1[i]*beta1 + x2[i]*beta2 #calculate yprob including only the population (fixed) effects.
  }
  ystar <- rlogis(n,yprob,1) # var(ystar - yprob) = 3.29
  for (i in 1:n){
    ystar[i] <- ystar[i] + uj[group[i]] #Add the group effect to ystar
  }
  yobs <- ifelse(ystar >= 0, 1, 0) #Get the binary outcome using the threshold test
  return(data.frame(x1, x2, group, yprob, ystar, yobs))
}

```


Now a dataset is generated, note the value of 1.5 for jstd, the standard deviation of the group intercepts. This corresponds to a variance of 2.25.

```{r}
mdata <- multibin(n = 5000, 
                  J = 20, 
                  beta1 = 1.6, #3.2
                  beta2 = -0.7,
                  jstd = 1.5)
```


```{r}
head(mdata)
```

Now let's plot the data, for a small subset of the groups (and a subsample within each group). Keep in mind that in a real study, ystar, being a latent variable, is invisible.

```{r}
mdata %>%
  filter(group %in% seq(1,4)) %>%
  mutate(Group = paste0("Group ", group)) %>%
  group_by(Group) %>%
  sample_n(100) %>%
  ggplot()+
  aes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+
  geom_hline(yintercept = 0, col = "grey", linetype = "dashed")+
  geom_point(alpha = 0.3)+
  geom_point(aes(x = x1, y = yobs), size = 1.5)+
  scale_colour_discrete(guide = FALSE)+
  scale_shape_discrete(name = "x2")+
  theme_bw(14)+
  facet_wrap(~Group)
  
```


The variance of the residuals should be approximately 3.29 + some variance attributable to the group-level effect. We can calculate the true variance from our simulation.

$$\sigma^{2}_{\text{total}}=\text{var}(y^*-y_{prob})$$

```{r}
var(mdata$ystar - mdata$yprob)
```

How much variance did the group level effect add?

$$\sigma^{2}_{\text{total}} = \sigma^{2}_{e*} + \sigma^{2}_{u}$$

Where $\sigma^{2}_{e*}$ is the variance attributable to the population (fixed) effects and $\sigma^{2}_{u}$ is the variance attributable to the group (random) effects.


```{r}
var(mdata$ystar - mdata$yprob) - (pi*pi)/3

```

What proportion of the total variance does this represent? I.e. what is the variance partition coefficient (VPC)?

$$ VPC = \frac{\sigma^{2}_{u}}{\sigma^{2}_{e*}+\sigma^{2}_{u}}$$



```{r}
(var(mdata$ystar - mdata$yprob) - (pi*pi)/3)/var(mdata$ystar - mdata$yprob)
```

About `r round(100*(var(mdata$ystar - mdata$yprob) - (pi*pi)/3)/var(mdata$ystar - mdata$yprob),0)`%.


Now that we know the true variance values to expect from the simulation, we can fit a multilevel logistic regression model and see how close it gets. Here, I use the function glmer from the package lme4, which uses a maximum likelihood estimator. One point Prof Tranmer made in his talk was that Bayesian Markov Chain Monte Carlo estimators may do a better job for this particular problem, however, I have simulated a lot of observations so it shouldn't really make a difference. If I wanted to use MCMC, the package brms can accept a formula in the same syntax as lme4.


```{r}
mmod1 <- glmer(yobs ~ x1 + x2 + (1 | group), 
              data = mdata, 
              family = "binomial")

summary(mmod1)
```

Looking at the summary, the variance attributable to the group (random effect) is pretty close to the true value. If this were a real model, the VPC of `r round(100*(var(mdata$ystar - mdata$yprob) - (pi*pi)/3)/var(mdata$ystar - mdata$yprob),0)`% would mean we would be certain to include the group effect. Now we can plot the varying interecepts model for $y^*$ (for the same subset of data as above). Each group has two lines, one for each value of the predictor x2.

```{r}
mdata$predicted <- predict(mmod1, mdata)

```


```{r}
mdata %>%
  filter(group %in% seq(1,4)) %>%
  mutate(Group = paste0("Group ", group)) %>%
  group_by(Group) %>%
  sample_n(100) %>%
  ggplot()+
  aes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+
  geom_hline(yintercept = 0, col = "grey", linetype = "dashed")+
  geom_point(alpha = 0.3)+
  geom_point(aes(x = x1, y = yobs), size = 1.5)+
  geom_line(aes(x = x1, y = predicted, group = interaction(x2,group)))+
  scale_colour_discrete(guide = FALSE)+
  scale_shape_discrete(name = "x2")+
  scale_x_continuous(limits = c(-2,2))+
  theme_bw(14)+
  facet_wrap(~Group)
```



## Reference
 Goldstein H, Browne W, Rasbash J. Partitioning variation in generalised linear multilevel models. Understanding Statistics 2002; 1:223â€“232

