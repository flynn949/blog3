---
title: Multilevel logistic regression (again)
author: Flynn Hill
date: '2018-11-26'
output: blogdown::html_page
slug: multilevel-logistic-regression
categories: []
tags:
  - multilevel modelling
  - logistic regression
  - variance
header:
  caption: ''
  image: ''
---



<p>I recently attended a whole-day workshop on multilevel modelling organised by <a href="https://niasra.uow.edu.au/index.html">NIASRA</a> and taught by Professor <a href="https://www.gla.ac.uk/schools/socialpolitical/staff/marktranmer/">Mark Tranmer</a>, who is an expert in the field from the University of Glasgow.</p>
<p>One part of the workshop that I realised I didn’t have any familiarity with was the idea of how variance works in logistic regression, and in particular in a multilevel logistic regression.</p>
<p>Here is one way of looking at logistic regression, in a formulation that reflects how a glm works:</p>
<p><span class="math display">\[ y_{prob} = \text{logit}^{-1}(X\beta) \\ y_{obs} \sim \text{Binomial}(y_{prob}) \]</span></p>
<p>Where <span class="math inline">\(\text{logit}^{-1}\)</span> is the inverse logit function: <span class="math inline">\(exp(X\beta)/(exp(X\beta) + 1)\)</span> which transforms the linear predictor <span class="math inline">\(X\beta\)</span> from log-odds to probabilities <span class="math inline">\(y_{prob}\)</span>. These then give the observed outcomes <span class="math inline">\(y_{obs}\)</span> according to a binomial distribution.</p>
<p>In multilevel logistic regression, it turns out that adding group (random) effects increases the residual variance. This presents a challenge when we are trying to decide whether it is worth including a group effect. What can we say about how much of the total variance is explained by the group effect?</p>
<p>To explore this issue, the latent variable model is helpful. Although mathematically equivalent to the glm formulation above, this conceptualisation offers an easier way to look at the variance.</p>
<p>In the latent variable model, there is an underlying linear variable <span class="math inline">\(y^*\)</span>, which produces a <span class="math inline">\(y_{obs}\)</span> value of 1 when it is above 0 and 0 when it is below 0. <span class="math inline">\(y^*\)</span> is given by a predictor with standard logistic distribution, centred at a value given by a linear predictor, with a scale of 1. This distribution has a variance of (pi^2)/3 = 3.29.</p>
<p><span class="math display">\[ y_{prob}=X\beta \\ y^* \sim \text{logistic}(y_{prob}, 1) \\ y_{obs}=y^*\geq0\;?\; 1 : 0 \]</span></p>
<p>The logistic distribution has a cumulative distribution function given by</p>
<p><span class="math display">\[ \frac{1}{2} + \frac{1}{2}\text{tanh}(\frac{x-\mu}{2s})\]</span></p>
<p>The CDF of a logistic distribution (black) with scale parameter <span class="math inline">\(s=1\)</span> is almost identical to a normal distribution with a variance of 3.29 (red).</p>
<pre class="r"><code>x &lt;- seq(-10,10,0.01)
pl &lt;- plogis(x, location = 0, scale = 1)
nl&lt;- pnorm(x, mean = 0, sd = sqrt(pi*pi/3))
plot(x,pl,t=&quot;l&quot;)
lines(x, nl, col=&quot;red&quot;)</code></pre>
<p><img src="/post/2018-11-26-multilevel-logistic-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>How does this apply to multilevel models? We can put all population effects into <span class="math inline">\(y_{prob}\)</span> and calculate <span class="math inline">\(y^*\)</span> from a logistic distribution centred on <span class="math inline">\(y_{prob}\)</span>, plus an additional intercept term <span class="math inline">\(u_j\)</span> for the group effect, which stretches out <span class="math inline">\(y^*\)</span>:</p>
<p><span class="math display">\[ y_{prob}=X\beta \\ y^* \sim \text{logistic}(y_{prob}, 1)+u_j \\ y_{obs}=y^*\geq0\;?\; 1 : 0 \]</span></p>
<p>Thus, the population effects are set to contribute a variance of 3.29, while any group effects are added on top of this variance. If we subtract 3.29 from the total residual variance, we can find the variance attributable to the group effect, and take this as a proportion of the total variance.</p>
<div id="simulation" class="section level2">
<h2>Simulation</h2>
<pre class="r"><code>set.seed(42)</code></pre>
<p>Here, I simulate a dataset by way of the latent variable formulation above. This allows us to know the true proportion of variance explained by the group effect.</p>
<p>Setup:</p>
<ul>
<li>Binary outcome yobs</li>
<li>One continuous predictor x1</li>
<li>One binary predictor x2</li>
<li>A group level effect with varying intercepts uj.</li>
</ul>
<pre class="r"><code>multibin &lt;- function(n, J, beta1, beta2, jstd){
  x1 &lt;- rnorm(n,0,1) #standardised continuous predictor variable
  x2 &lt;- sample(c(0,1),n,replace=TRUE) #binary predictor variable
  group &lt;- sample(J, n, replace=TRUE) #group membership indicator
  uj &lt;- rnorm(J,0,jstd) #Group intercepts, normally distributed with standard deviation jstd
  yprob &lt;- rep(NA,n)
  for (i in 1:n){
    yprob[i] &lt;- x1[i]*beta1 + x2[i]*beta2 #calculate yprob including only the population (fixed) effects.
  }
  ystar &lt;- rlogis(n,yprob,1) # var(ystar - yprob) = 3.29
  for (i in 1:n){
    ystar[i] &lt;- ystar[i] + uj[group[i]] #Add the group effect to ystar
  }
  yobs &lt;- ifelse(ystar &gt;= 0, 1, 0) #Get the binary outcome using the threshold test
  return(data.frame(x1, x2, group, yprob, ystar, yobs))
}</code></pre>
<p>Now a dataset is generated, note the value of 1.5 for jstd, the standard deviation of the group intercepts. This corresponds to a variance of 2.25.</p>
<pre class="r"><code>mdata &lt;- multibin(n = 5000, 
                  J = 20, 
                  beta1 = 1.6, #3.2
                  beta2 = -0.7,
                  jstd = 1.5)</code></pre>
<pre class="r"><code>head(mdata)</code></pre>
<pre><code>##           x1 x2 group       yprob      ystar yobs
## 1  1.3709584  1    13  1.49353352  1.3347024    1
## 2 -0.5646982  1    17 -1.60351707 -2.9394443    0
## 3  0.3631284  1     7 -0.11899454  2.9076940    1
## 4  0.6328626  0     8  1.01258017  0.6882657    1
## 5  0.4042683  1     1 -0.05317068  1.9769707    1
## 6 -0.1061245  1     4 -0.86979923 -3.6911985    0</code></pre>
<p>Now let’s plot the data, for a small subset of the groups (and a subsample within each group).</p>
<pre class="r"><code>mdata %&gt;%
  filter(group %in% seq(1,4)) %&gt;%
  mutate(Group = paste0(&quot;Group &quot;, group)) %&gt;%
  group_by(Group) %&gt;%
  sample_n(100) %&gt;%
  ggplot()+
  aes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+
  geom_hline(yintercept = 0, col = &quot;grey&quot;, linetype = &quot;dashed&quot;)+
  geom_point(alpha = 0.3)+
  geom_point(aes(x = x1, y = yobs), size = 1.5)+
  scale_colour_discrete(guide = FALSE)+
  scale_shape_discrete(name = &quot;x2&quot;)+
  theme_bw(14)+
  facet_wrap(~Group)</code></pre>
<p><img src="/post/2018-11-26-multilevel-logistic-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The variance of the residuals should be approximately 3.29 + some variance attributable to the group-level effect. We can calculate the true variance from our simulation.</p>
<p><span class="math display">\[\sigma^{2}_{\text{total}}=\text{var}(y^*-y_{prob})\]</span></p>
<pre class="r"><code>var(mdata$ystar - mdata$yprob)</code></pre>
<pre><code>## [1] 5.380638</code></pre>
<p>How much variance did the group level effect add?</p>
<p><span class="math display">\[\sigma^{2}_{\text{total}} = \sigma^{2}_{e*} + \sigma^{2}_{u}\]</span></p>
<p>Where <span class="math inline">\(\sigma^{2}_{e*}\)</span> is the variance attributable to the population (fixed) effects and <span class="math inline">\(\sigma^{2}_{u}\)</span> is the variance attributable to the group (random) effects.</p>
<pre class="r"><code>var(mdata$ystar - mdata$yprob) - (pi*pi)/3</code></pre>
<pre><code>## [1] 2.09077</code></pre>
<p>What proportion of the total variance does this represent? I.e. what is the variance partition coefficient (VPC)?</p>
<p><span class="math display">\[ VPC = \frac{\sigma^{2}_{u}}{\sigma^{2}_{e*}+\sigma^{2}_{u}}\]</span></p>
<pre class="r"><code>(var(mdata$ystar - mdata$yprob) - (pi*pi)/3)/var(mdata$ystar - mdata$yprob)</code></pre>
<pre><code>## [1] 0.3885728</code></pre>
<p>About 60%.</p>
<p>Now that we know the true variance values to expect from the simulation, we can fit a multilevel logistic regression model and see how close it gets. Here, I use the function glmer from the package lme4, which uses a maximum likelihood estimator. One point Prof Tranmer made in his talk was that Bayesian Markov Chain Monte Carlo estimators may do a better job for this particular problem, however, I have simulated a lot of observations so it shouldn’t really make a difference. If I wanted to use MCMC, the package brms can accept a formula in the same syntax as lme4.</p>
<pre class="r"><code>mmod1 &lt;- glmer(yobs ~ x1 + x2 + (1 | group), 
              data = mdata, 
              family = &quot;binomial&quot;)

summary(mmod1)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: yobs ~ x1 + x2 + (1 | group)
##    Data: mdata
## 
##      AIC      BIC   logLik deviance df.resid 
##   4343.1   4369.2  -2167.6   4335.1     4996 
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -14.2165  -0.4928  -0.1588   0.4875  12.4820 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  group  (Intercept) 2.064    1.437   
## Number of obs: 5000, groups:  group, 20
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.25247    0.32597  -0.775    0.439    
## x1           1.64242    0.05400  30.417  &lt; 2e-16 ***
## x2          -0.63296    0.07725  -8.194 2.53e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##    (Intr) x1    
## x1 -0.012       
## x2 -0.114 -0.117</code></pre>
<p>Looking at the summary, the variance attributable to the group (random effect) is pretty close to the true value. If this were a real model, the VPC of 0.6 would mean we would be certain to include the group effect. Now we can plot the varying interecepts model for <span class="math inline">\(y^*\)</span> (for the same subset of data as above). Each group has two lines, one for each value of the predictor x2.</p>
<pre class="r"><code>mdata$predicted &lt;- predict(mmod1, mdata)</code></pre>
<pre class="r"><code>mdata %&gt;%
  filter(group %in% seq(1,4)) %&gt;%
  mutate(Group = paste0(&quot;Group &quot;, group)) %&gt;%
  group_by(Group) %&gt;%
  sample_n(100) %&gt;%
  ggplot()+
  aes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+
  geom_hline(yintercept = 0, col = &quot;grey&quot;, linetype = &quot;dashed&quot;)+
  geom_point(alpha = 0.3)+
  geom_point(aes(x = x1, y = yobs), size = 1.5)+
  geom_line(aes(x = x1, y = predicted, group = interaction(x2,group)))+
  scale_colour_discrete(guide = FALSE)+
  scale_shape_discrete(name = &quot;x2&quot;)+
  scale_x_continuous(limits = c(-2,2))+
  theme_bw(14)+
  facet_wrap(~Group)</code></pre>
<p><img src="/post/2018-11-26-multilevel-logistic-regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<p>Goldstein H, Browne W, Rasbash J. Partitioning variation in generalised linear multilevel models. Understanding Statistics 2002; 1:223–232</p>
</div>
