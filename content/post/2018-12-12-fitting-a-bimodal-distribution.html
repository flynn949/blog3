---
title: Fitting a Bimodal Distribution
author: ''
date: '2018-12-12'
slug: fitting-a-bimodal-distribution
categories: []
tags:
  - stan
  - bimodal
  - fitting
header:
  caption: ''
  image: ''
---



<p>As a biophysicist, I occasionally found myself fitting Gaussian curves to apparently bimodal distributions. For example, in one experiment I mixed the relatively slow T7 bacteriophage polymerase with the fast <em>E. coli</em> polymerase, and measured rates of DNA replication at the single molecule level for hundreds of the molecules in this mixture. It was clear to see that the result was a bimodal distribution, but I didn’t have a great method for fitting this - I used graphing software (Origin) that spat out its best estimate, but I did not know what was going on under the hood, and had no feel for how good the fit was, or the range of fits that would be consistent with the data.</p>
<p>I think the Bayesian method used below is much nicer. It might be a bit more involved and slower, but if you have put all the effort in to get the data, it is worth doing the analysis well.</p>
<p>First, some data are simulated. There are five key parameters used in the simulation, which the model will seek to retrieve: mu[1], mu[2], sigma[1], sigma[2] and lambda. Lambda is a value between 0 and 1, it is the ratio of occupancy of the two peaks.</p>
<pre class="r"><code>set.seed(42)
mu &lt;- c(300,460)
sigma &lt;- c(50,100)
lambda &lt;- 0.57
nobs &lt;- 500
#assign an underlying distribution for each observation.
#The probability of being in one distribution or the other is given by lambda.
xgroup &lt;- sample(c(1,2), nobs, prob = c(lambda, 1 - lambda), replace = TRUE) 
#generate the set of observations
x &lt;- rnorm(nobs, mu[xgroup], sigma[xgroup])</code></pre>
<pre class="r"><code>hist(x, breaks = 25)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Here is the Stan code, this is actually stored in a separate file, ‘bimodal.stan’. I struggled to get this to work. My initial strategy was to have separate priors for each of the two mu values, with one higher than the other. Even so, the chains would still flip-flop from mu[1] being higher to mu[2] being higher. Then I came across <a href="http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">this</a> extremely detailed and helpful post by Michael Betancourt that provides the solution. If mu is set to be of the type ‘ordered’, then it ensures mu[1] is always the smaller value. Simple!</p>
<pre class="c"><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
}
transformed data {
  vector[N] x_std;
  x_std = (x - mean(x))/sd(x);
}
parameters {
  ordered[2] mu_std;
  vector&lt;lower=0&gt;[2] sd_std;
  real&lt;lower=0,upper=1&gt; lambda;
}
model {
  mu_std ~ normal(0,4);
  sd_std ~ cauchy(0,5);
  lambda ~ beta(5,5);
  for (n in 1:N){
    target += log_mix(lambda,
    normal_lpdf(x_std[n] | mu_std[1], sd_std[1]),
    normal_lpdf(x_std[n] | mu_std[2], sd_std[2]));
  }
}
generated quantities {
  vector[2] mu;
  vector[2] std;
  mu = (sd(x)  * mu_std) + mean(x);
  std = sd(x)*sd_std;
}
</code></pre>
<p>The Stan model is run with four chains. My laptop is 7 years old so I only run it on one core, but it still goes OK.</p>
<p>To explain it in a nutshell: each chain explores the posterior density distribution by Markov chain Monte Carlo sampling. In this method, each chain iteratively explores the parameter space and fills up a nascent picture of what the posterior density looks like. In each step, a set of candidate parameter values are evaluated based on the likelihood of their generating the observed data multiplied by their prior probability. The probability of accepting these candidate parameters is then given by the ratio of this value to that of the value for the previously accepted set of values. If this ratio is 0.5, for example, there is a 50% chance of the new values being accepted - thus there is an element of stochasticity. The chain then jumps off from the most recently accepted set of parameter values to a new point in parameter space, to evaluate a new set of parameters. The amount of time (number of iterations) the chain spends in each region of parameter space is thus proportional to the probability density in that region. After a defined number of iterations, the exploration of the posterior density ceases. This is replicated over a few different chains, which each start at a different set of initial values. We would hope that these chains are in good agreement.</p>
<pre class="r"><code>#Fit the model, as specified in the file &#39;bimodal.stan&#39;.
library(rstan)
mod &lt;- stan(file = &quot;bimodal.stan&quot;,data = list(N = length(x), x = x),
            chains = 4, cores = 1, warmup = 300, iter = 1000,verbose = FALSE, refresh = -1)</code></pre>
<pre class="r"><code>plot(mod, pars = &quot;mu&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>plot(mod, pars = &quot;std&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>plot(mod, pars=&quot;lambda&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>And here is a summary of the point estimates and credibility intervals for the five parameters. If I were reporting this fit in a publication, I would at a minimum quote the mean values and the 95% intervals. Additionally, the data and code should be made available.</p>
<p>Note the rhat values close to 1, these indicate close agreement among the chains.</p>
<pre class="r"><code>summary(mod, pars = c(&quot;mu&quot;, &quot;std&quot;, &quot;lambda&quot;), probs = c(0.025, 0.975))$`summary`</code></pre>
<pre><code>##               mean     se_mean          sd        2.5%       97.5%
## mu[1]  295.0707693 0.124435066  4.36025314 286.7992427 303.7104136
## mu[2]  453.2434491 0.572382756 16.93860372 421.0960779 485.9485087
## std[1]  47.3655734 0.103262768  3.42673783  40.5079441  54.2936518
## std[2]  99.2645647 0.258848017  8.71884238  82.0606036 115.9580105
## lambda   0.5766897 0.001766867  0.05335869   0.4690794   0.6744591
##            n_eff     Rhat
## mu[1]  1227.8289 1.001190
## mu[2]   875.7540 1.002667
## std[1] 1101.2204 1.002293
## std[2] 1134.5613 1.000581
## lambda  912.0162 1.001420</code></pre>
<p>To get a better sense of what is going on under the hood and see what the posterior ‘density’ looks like, the median fit has been plotted in the two thick lines, with a random selection of 100 posterior draws (a draw constitutes a combination of the five variables of interest, as given by a single iteration of a single chain) plotted in dotted lines.</p>
<pre class="r"><code>draws &lt;- extract(mod)
xrange &lt;- seq(0,1000)

x1.line &lt;- dnorm(xrange, median(draws$mu[,1]), median(draws$std[,1]))*(median(draws$lambda))
x2.line &lt;- dnorm(xrange, median(draws$mu[,2]), median(draws$std[,2]))*median(1 - draws$lambda)

drawsamples &lt;- sample(seq(1:length(draws$mu[,1])), 100)

hist(x, freq = FALSE, xlim = c(0,1000), ylim = c(0,0.007), breaks = 25)
for (i in seq(1:100)){
  for (j in c(1,2)){
    lbda &lt;- ifelse(j == 2, 1 - draws$lambda[drawsamples[i]], draws$lambda[drawsamples[i]])
    line &lt;- dnorm(xrange, draws$mu[,j][drawsamples[i]], draws$std[,j][drawsamples[i]])*lbda
    lines(xrange, line, col = j, cex = 0.8, lty = 3)
  }
}
lines(xrange,x1.line, lwd = 3, col = &quot;lightblue&quot;)
lines(xrange,x2.line, lwd = 3, col = &quot;orange&quot;)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
