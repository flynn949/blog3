---
title: Fitting a Bimodal Distribution
author: ''
date: '2018-12-12'
slug: fitting-a-bimodal-distribution
categories: []
tags:
  - stan
  - bimodal
  - fitting
header:
  caption: ''
  image: ''
---



<p>As a biophysicist, I occasionally found myself fitting Gaussian curves to apparently bimodal distributions. For example, in one experiment I mixed the relatively slow T7 bacteriophage polymerase with the fast <em>E. coli</em> polymerase, and measured rates of DNA replication at the single molecule level for hundreds of the molecules in this mixture. It was clear to see that the result was a bimodal distribution, but I didn’t have a great method for fitting this - I used graphing software (Origin) that spat out its best estimate, but I did not know what was going on under the hood, and had no feel for how good the fit was, or the range of fits that would be consistent with the data.</p>
<p>I think the Bayesian method used below is much nicer. It might be a bit more involved and slower, but I think if you have put all the effort in to get the data, it is worth doing the analysis well.</p>
<pre class="r"><code>#Simulate data
set.seed(42)
mu &lt;- c(300,460)
sigma &lt;- c(50,100)
lambda &lt;- 0.57
xgroup &lt;- sample(c(1,2), 10000, prob = c(lambda, 1 - lambda), replace = TRUE)
x &lt;- rnorm(500, mu[xgroup], sigma[xgroup])</code></pre>
<pre class="r"><code>hist(x, breaks = 25)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Here is the Stan code, this is actually stored in a separate file, ‘bimodal.stan’. I struggled to get this to work. My initial strategy was to have separate priors for each of the two mu values, with one higher than the other. Even so, the chains would still flip-flop from mu[1] being higher to mu[2] being higher. Then I came across <a href="http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">this</a> extremely detailed and helpful post by Michael Betancourt that provides the solution. If mu is set to be of the type ‘ordered’, then it ensures mu[1] is always the smaller value. Simple!</p>
<pre class="c"><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
}
transformed data {
  vector[N] x_std;
  x_std = (x - mean(x))/sd(x);
}
parameters {
  ordered[2] mu_std;
  vector&lt;lower=0&gt;[2] sd_std;
  real&lt;lower=0,upper=1&gt; lambda;
}
model {
  mu_std ~ normal(0,4);
  sd_std ~ cauchy(0,5);
  lambda ~ beta(5,5);
  for (n in 1:N){
    target += log_mix(lambda,
    normal_lpdf(x_std[n] | mu_std[1], sd_std[1]),
    normal_lpdf(x_std[n] | mu_std[2], sd_std[2]));
  }
}
generated quantities {
  vector[2] mu;
  vector[2] std;
  mu = (sd(x)  * mu_std) + mean(x);
  std = sd(x)*sd_std;
}
</code></pre>
<p>The Stan model is run with four chains. My laptop is 7 years old so I only run it on one core, but it still goes OK.</p>
<pre class="r"><code>#Fit the model, as specified in the file &#39;bimodal.stan&#39;.
library(rstan)
mod &lt;- stan(file = &quot;bimodal.stan&quot;,data = list(N = length(x), x = x),
            chains = 4, cores = 1, warmup = 300, iter = 1000,verbose = FALSE, refresh = -1)</code></pre>
<pre class="r"><code>plot(mod, pars = &quot;mu&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>plot(mod, pars = &quot;std&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>plot(mod, pars=&quot;lambda&quot;)</code></pre>
<pre><code>## ci_level: 0.8 (80% intervals)</code></pre>
<pre><code>## outer_level: 0.95 (95% intervals)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Below, the median fit has been plotted in the two thick lines, with a random selection of 100 draws plotted in dotted lines to give a feel for what the posterior density looks like in relation to the data. In other situations, a posterior predictive distribution might be plotted instead, but here there are no predictor variables, so I prefer this.</p>
<pre class="r"><code>draws &lt;- extract(mod)
xrange &lt;- seq(0,1000)

x1.line &lt;- dnorm(xrange, median(draws$mu[,1]), median(draws$std[,1]))*(median(draws$lambda))
x2.line &lt;- dnorm(xrange, median(draws$mu[,2]), median(draws$std[,2]))*median(1 - draws$lambda)

drawsamples &lt;- sample(seq(1:length(draws$mu[,1])), 100)

hist(x, freq = FALSE, xlim = c(0,1000), ylim = c(0,0.007), breaks = 25)
for (i in seq(1:100)){
  for (j in c(1,2)){
    lbda &lt;- ifelse(j == 2, 1 - draws$lambda[drawsamples[i]], draws$lambda[drawsamples[i]])
    line &lt;- dnorm(xrange, draws$mu[,j][drawsamples[i]], draws$std[,j][drawsamples[i]])*lbda
    lines(xrange, line, col = j, cex = 0.8, lty = 3)
  }
}
lines(xrange,x1.line, lwd = 3, col = &quot;lightblue&quot;)
lines(xrange,x2.line, lwd = 3, col = &quot;orange&quot;)</code></pre>
<p><img src="/post/2018-12-12-fitting-a-bimodal-distribution_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
