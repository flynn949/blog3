---
title: "Quantifying an educational effect"
author: "Flynn Hill"
date: 2018-09-13
output: blogdown::html_page
tags: ["regression", "statistics", "power analysis"]
---



<p>Say we have a cohort of 500 first-year students commencing their second session of study. We decide to offer a special tutoring service to some of the students who did not go well in their first session. To do this, we arrange students in ascending order of their averaged first session final marks, and go down the list calling students and offering tutoring services until 50 students have taken up the offer. At the end of the second session, we want to see if this made a difference. Here we will consider a simple situation where all relevant variables have been taken into account, and the effect is therefore consistent for all students - if only real life were so simple!</p>
<p>Although I am usually a devotee of the tidyverse, to keep things simple and focus on the procedure, I have used only base R in this post, and pretty much only used vectors - no data frames!</p>
<div id="simulation" class="section level3">
<h3>Simulation</h3>
<p>Start with the basic parameters.</p>
<pre class="r"><code>set.seed(42)
n_students &lt;- 500
n_tutoring &lt;- 50</code></pre>
<p>Generate a reasonable looking distribution of first-session marks. This should be bounded by zero and 100. One way to do this is to make the distribution in the log odds (logit) scale (which is unbounded), and then transform back to the marks scale. This will require functions for transforming to and from the logit scale.</p>
<pre class="r"><code>invlogit &lt;- function(x){1/(1+exp(-x))}

logit &lt;- function(x){log(x/(1-x))}

first_session_marks &lt;- sort(100*invlogit(rnorm(n_students, logit(60/100), 0.7)))

hist(first_session_marks)</code></pre>
<p><img src="/post/outcomespost_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Now we start calling students to offer them tutoring, starting from those with the lowest first session marks. Here, the probability of a student accepting tutoring is 0.8 - first_session_mark/100, i.e. students with lower marks are more likely to accept tutoring.</p>
<pre class="r"><code>n_tutoring_places_accepted &lt;- 0
tutored &lt;- rep(0, n_students)
i &lt;- 1
while (n_tutoring_places_accepted &lt; n_tutoring &amp; i &lt; n_students) {
 rval &lt;- runif(1) 
 if (rval &lt; 0.8 - first_session_marks[i]/100){
   tutored[[i]] &lt;- 1
   n_tutoring_places_accepted &lt;- n_tutoring_places_accepted + 1
 }
 i &lt;- i + 1
}

sum(tutored)</code></pre>
<pre><code>## [1] 50</code></pre>
<p>Now simulate second session marks. These should be tightly correlated to first session marks, and will be reasonably similar. In this model,there is an intercept term that adds a small value to the first session mark, a slope term that scales the first session mark, and another interept term for the effect associated with tutoring. Realised values of the second session mark are then drawn from Gaussian distributions centred at these calculated mark values, with a defined standard deviation that is the same for all students. Thus, for student <span class="math inline">\(i\)</span> we have:</p>
<p><span class="math display">\[\text{Second session mark}_i \sim \text{Normal}(\mu_i,\sigma)\\ \mu_i = \beta_0 + \beta_1\text{first_session_mark}_i + \beta_2\text{Tutored}_i \]</span> Define the parameters, and generate the second session marks according to the model.</p>
<pre class="r"><code>beta_0 &lt;- 2
beta_1 &lt;- 0.9
beta_2 &lt;- 3
sigma &lt;- 5

second_session_marks.mu &lt;- beta_0 + beta_1*first_session_marks + beta_2*tutored
second_session_marks &lt;- rnorm(n_students, second_session_marks.mu, sigma)


hist(second_session_marks)</code></pre>
<p><img src="/post/outcomespost_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Plot the first and second session marks, coloured by whether tutoring was accepted.</p>
<pre class="r"><code>tutored.f &lt;- factor(tutored, levels = c(0,1))
plot(first_session_marks, second_session_marks, col = tutored.f)</code></pre>
<p><img src="/post/outcomespost_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Students who received tutoring have had a small but appreciable bump in their marks. To measure the bump in marks, fit a regression model that looks just like the one used to create the data.</p>
<pre class="r"><code>regression_model &lt;- lm(second_session_marks ~ 1 + first_session_marks + tutored)
summary(regression_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = second_session_marks ~ 1 + first_session_marks + 
##     tutored)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -16.7165  -3.6281   0.1935   3.5529  17.6140 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          1.58472    1.12579   1.408  0.15986    
## first_session_marks  0.90359    0.01796  50.305  &lt; 2e-16 ***
## tutored              2.50125    0.89768   2.786  0.00553 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.147 on 497 degrees of freedom
## Multiple R-squared:  0.8683, Adjusted R-squared:  0.8677 
## F-statistic:  1638 on 2 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<p> </p>
<p>The model has retrieved the simuation parameters quite nicely - no surprise, they are a lot less noisy than real data, all relevant variables have been included, and the effect size is appreciable and consistent.</p>
<p> </p>
</div>
<div id="power-analysis-by-simulation" class="section level3">
<h3>Power analysis by simulation</h3>
<p>A good idea before doing a study like this is to calculte its power - the probability of rejecting the null hypothesis when teh null hypothesis is false (at a given significance level). This calculation can either help us to decide how many study subjects are needed to reach a desired power level, or if we are unable to increase the number of study subjects it can at least give us a realistic appreciation for the power of our study.</p>
<p>This uses a simpler model for which students receive tutoring - for the bottom 100 students, each has a 50% chance of accepting tutoring (up to a maximum of 50 tutoring places). We can use this to find the power for an estimated effect size, or to find the minimum effect size needed to achieve a given power. By replicating the above simulation thousands of times, we can quantify the fraction for which the p-value of the beta_3 coefficient was below the significance level.</p>
<pre class="r"><code>power_simulation &lt;- function(n_tutored = 50, n_untutored = 450, beta3, sigma_secondsession, beta0 = 2, beta1 = 0.9){
  nstudents &lt;- n_tutored + n_untutored 
  first_session &lt;- sort(100*invlogit(rnorm(nstudents, logit(60/100), 0.7)))  
  second_session &lt;- beta0 + beta1*first_session + rnorm(nstudents, 0, sigma_secondsession)
  tutored.indices &lt;- sample(seq(1,n_tutored*2), n_tutored)
  tutored &lt;- rep(0, nstudents)
  tutored[tutored.indices] &lt;- 1
  second_session[tutored.indices] &lt;- second_session[tutored.indices] + beta3
  lmod &lt;- summary(lm(second_session ~ first_session + tutored))
  return(lmod$coefficients[3,4]) #The pvalue for the beta3 estimate.
}


pvals &lt;- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, beta3 = 2.4, sigma_secondsession = 5))

mean(pvals &lt;= 0.05)</code></pre>
<pre><code>## [1] 0.7948</code></pre>
<p>So, we have a calculated power of 0.7948. It is also interesting to look at the distribution of these p-values, in the logit scale.</p>
<pre class="r"><code>hist(logit(pvals),breaks = 50)
abline(v = logit(0.05), col = &quot;red&quot;) #The vertical line is at p = 0.05</code></pre>
<p><img src="/post/outcomespost_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Why is the distribution shaped in this particular way? I don’t know - I will have to read up on this.</p>
<p> </p>
</div>
<div id="concluding-remark" class="section level3">
<h3>Concluding remark</h3>
<p>Going into a power analysis it is common to have a personal bias to thinking our effect size will be greater than it really is, and that the data are less noisy than they really are. Without going too much into the whys and wherefores of power analysis and null hypothesis significance testing, <a href="http://www.stat.columbia.edu/~gelman/research/published/PPS551642_REV2.pdf">it is good practice</a> to use a conservative estimate of the effect size based on external information (e.g. prior studies).</p>
<p> </p>
</div>
