[{"authors":null,"categories":[],"content":"As a biophysicist, I occasionally found myself fitting Gaussian curves to apparently bimodal distributions. For example, in one experiment I mixed the relatively slow T7 bacteriophage polymerase with the fast E. coli polymerase, and measured rates of DNA replication at the single molecule level for hundreds of the molecules in this mixture. It was clear to see that the result was a bimodal distribution, but I didn’t have a great method for fitting this - I used graphing software (Origin) that spat out its best estimate, but I did not know what was going on under the hood, and had no feel for how good the fit was, or the range of fits that would be consistent with the data.\nI think the Bayesian method used below is much nicer. It might be a bit more involved and slower, but I think if you have put all the effort in to get the data, it is worth doing the analysis well.\nFirst, some data are simulated. There are five key parameters used in the simulation, which the model will seek to retrieve: mu[1], mu[2], sigma[1], sigma[2] and lambda. Lambda is a value between 0 and 1, it is the ratio of peak heights.\nset.seed(42)\rmu \u0026lt;- c(300,460)\rsigma \u0026lt;- c(50,100)\rlambda \u0026lt;- 0.57\rnobs \u0026lt;- 500\r#assign an underlying distribution for each observation.\r#The probability of being in one distribution or the other is given by lambda.\rxgroup \u0026lt;- sample(c(1,2), nobs, prob = c(lambda, 1 - lambda), replace = TRUE) #generate the set of observations\rx \u0026lt;- rnorm(nobs, mu[xgroup], sigma[xgroup])\rhist(x, breaks = 25)\rHere is the Stan code, this is actually stored in a separate file, ‘bimodal.stan’. I struggled to get this to work. My initial strategy was to have separate priors for each of the two mu values, with one higher than the other. Even so, the chains would still flip-flop from mu[1] being higher to mu[2] being higher. Then I came across this extremely detailed and helpful post by Michael Betancourt that provides the solution. If mu is set to be of the type ‘ordered’, then it ensures mu[1] is always the smaller value. Simple!\ndata {\rint\u0026lt;lower=0\u0026gt; N;\rvector[N] x;\r}\rtransformed data {\rvector[N] x_std;\rx_std = (x - mean(x))/sd(x);\r}\rparameters {\rordered[2] mu_std;\rvector\u0026lt;lower=0\u0026gt;[2] sd_std;\rreal\u0026lt;lower=0,upper=1\u0026gt; lambda;\r}\rmodel {\rmu_std ~ normal(0,4);\rsd_std ~ cauchy(0,5);\rlambda ~ beta(5,5);\rfor (n in 1:N){\rtarget += log_mix(lambda,\rnormal_lpdf(x_std[n] | mu_std[1], sd_std[1]),\rnormal_lpdf(x_std[n] | mu_std[2], sd_std[2]));\r}\r}\rgenerated quantities {\rvector[2] mu;\rvector[2] std;\rmu = (sd(x) * mu_std) + mean(x);\rstd = sd(x)*sd_std;\r}\r\rThe Stan model is run with four chains. My laptop is 7 years old so I only run it on one core, but it still goes OK. Four independent chains are run. Each chain explores the posterior density distribution by Markov chain Monte Carlo sampling. After initialising with a random combination of the five parameters, the likelihood of the data being generated from a distribution with these five parameters is calculated and multiplied by the prior probability to obtain the posterior probability. The probability of accepting this point in the paramter space in the posterior density is then proportional to this posterior probability. The chain then jumps to another position and repeats the process. Stan uses Hamiltonian Monte Carlo sampling, which has an efficient method for choosing the next place to jump. After initialisation, it takes a few dozen draws to equilibrate to the posterior density, so the first 300 ‘warmup’ draws in each chain are discarded. At the end of the process, we would hope that our four chains are in good agreement.\n#Fit the model, as specified in the file \u0026#39;bimodal.stan\u0026#39;.\rlibrary(rstan)\rmod \u0026lt;- stan(file = \u0026quot;bimodal.stan\u0026quot;,data = list(N = length(x), x = x),\rchains = 4, cores = 1, warmup = 300, iter = 1000,verbose = FALSE, refresh = -1)\rplot(mod, pars = \u0026quot;mu\u0026quot;)\r## ci_level: 0.8 (80% intervals)\r## outer_level: 0.95 (95% intervals)\rplot(mod, pars = \u0026quot;std\u0026quot;)\r## ci_level: 0.8 (80% intervals)\r## outer_level: 0.95 (95% intervals)\rplot(mod, pars=\u0026quot;lambda\u0026quot;)\r## ci_level: 0.8 (80% intervals)\r## outer_level: 0.95 (95% intervals)\rAnd here is a summary of the point estimates and credibility intervals for the five parameters. If I were reporting this fit in a publication, I would at a minimum quote the mean values and the 95% intervals. Additionally, the data and code should be made available.\nNote the rhat values close to 1, these indicate close agreement among the chains.\nsummary(mod, pars = c(\u0026quot;mu\u0026quot;, \u0026quot;std\u0026quot;, \u0026quot;lambda\u0026quot;), probs = c(0.025, 0.975))$`summary`\r## mean se_mean sd 2.5% 97.5%\r## mu[1] 295.0707693 0.124435066 4.36025314 286.7992427 303.7104136\r## mu[2] 453.2434491 0.572382756 16.93860372 421.0960779 485.9485087\r## std[1] 47.3655734 0.103262768 3.42673783 40.5079441 54.2936518\r## std[2] 99.2645647 0.258848017 8.71884238 82.0606036 115.9580105\r## lambda 0.5766897 0.001766867 0.05335869 0.4690794 0.6744591\r## n_eff Rhat\r## mu[1] 1227.8289 1.001190\r## mu[2] 875.7540 1.002667\r## std[1] 1101.2204 1.002293\r## std[2] 1134.5613 1.000581\r## lambda 912.0162 1.001420\rTo get a better sense of what is going on under the hood and see what the posterior ‘density’ looks like, the median fit has been plotted in the two thick lines, with a random selection of 100 posterior draws (a draw constitutes a combination of the five variables of interest, as given by a single iteration of a single chain) plotted in dotted lines.\ndraws \u0026lt;- extract(mod)\rxrange \u0026lt;- seq(0,1000)\rx1.line \u0026lt;- dnorm(xrange, median(draws$mu[,1]), median(draws$std[,1]))*(median(draws$lambda))\rx2.line \u0026lt;- dnorm(xrange, median(draws$mu[,2]), median(draws$std[,2]))*median(1 - draws$lambda)\rdrawsamples \u0026lt;- sample(seq(1:length(draws$mu[,1])), 100)\rhist(x, freq = FALSE, xlim = c(0,1000), ylim = c(0,0.007), breaks = 25)\rfor (i in seq(1:100)){\rfor (j in c(1,2)){\rlbda \u0026lt;- ifelse(j == 2, 1 - draws$lambda[drawsamples[i]], draws$lambda[drawsamples[i]])\rline \u0026lt;- dnorm(xrange, draws$mu[,j][drawsamples[i]], draws$std[,j][drawsamples[i]])*lbda\rlines(xrange, line, col = j, cex = 0.8, lty = 3)\r}\r}\rlines(xrange,x1.line, lwd = 3, col = \u0026quot;lightblue\u0026quot;)\rlines(xrange,x2.line, lwd = 3, col = \u0026quot;orange\u0026quot;)\r","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544572800,"objectID":"567b3a0c62421e2b052bcb025a551822","permalink":"/post/fitting-a-bimodal-distribution/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/fitting-a-bimodal-distribution/","section":"post","summary":"As a biophysicist, I occasionally found myself fitting Gaussian curves to apparently bimodal distributions. For example, in one experiment I mixed the relatively slow T7 bacteriophage polymerase with the fast E. coli polymerase, and measured rates of DNA replication at the single molecule level for hundreds of the molecules in this mixture. It was clear to see that the result was a bimodal distribution, but I didn’t have a great method for fitting this - I used graphing software (Origin) that spat out its best estimate, but I did not know what was going on under the hood, and had no feel for how good the fit was, or the range of fits that would be consistent with the data.","tags":["stan","bimodal","fitting"],"title":"Fitting a Bimodal Distribution","type":"post"},{"authors":null,"categories":[],"content":"I recently attended a whole-day workshop on multilevel modelling organised by NIASRA and taught by Professor Mark Tranmer, who is an expert in the field from the University of Glasgow.\nOne part of the workshop that I realised I didn’t have any familiarity with was the idea of how variance works in logistic regression, and in particular in a multilevel logistic regression.\nHere is one way of looking at logistic regression, in a formulation that reflects how a glm works:\n\\[ y_{prob} = \\text{logit}^{-1}(X\\beta) \\\\ y_{obs} \\sim \\text{Binomial}(y_{prob}) \\]\nWhere \\(\\text{logit}^{-1}\\) is the inverse logit function: \\(exp(X\\beta)/(exp(X\\beta) + 1)\\) which transforms the linear predictor \\(X\\beta\\) from log-odds to probabilities \\(y_{prob}\\). These then give the observed outcomes \\(y_{obs}\\) according to a binomial distribution.\nIn multilevel logistic regression, it turns out that adding group (random) effects increases the residual variance. This presents a challenge when we are trying to decide whether it is worth including a group effect. What can we say about how much of the total variance is explained by the group effect?\nTo explore this issue, the latent variable model is helpful. Although mathematically equivalent to the glm formulation above, this conceptualisation offers an easier way to look at the variance.\nIn the latent variable model, there is an underlying linear variable \\(y^*\\), which produces a \\(y_{obs}\\) value of 1 when it is above 0 and 0 when it is below 0. \\(y^*\\) is given by a predictor with standard logistic distribution, centred at a value given by a linear predictor, with a scale of 1. This distribution has a variance of (pi^2)/3 = 3.29.\n\\[ y_{prob}=X\\beta \\\\ y^* \\sim \\text{logistic}(y_{prob}, 1) \\\\ y_{obs}=y^*\\geq0\\;?\\; 1 : 0 \\]\nThe logistic distribution has a cumulative distribution function given by\n\\[ \\frac{1}{2} + \\frac{1}{2}\\text{tanh}(\\frac{x-\\mu}{2s})\\]\nThe CDF of a logistic distribution (black) with scale parameter \\(s=1\\) is almost identical to a normal distribution with a variance of 3.29 (red).\nx \u0026lt;- seq(-10,10,0.01)\rpl \u0026lt;- plogis(x, location = 0, scale = 1)\rnl\u0026lt;- pnorm(x, mean = 0, sd = sqrt(pi*pi/3))\rplot(x,pl,t=\u0026quot;l\u0026quot;)\rlines(x, nl, col=\u0026quot;red\u0026quot;)\rHow does this apply to multilevel models? We can put all population effects into \\(y_{prob}\\) and calculate \\(y^*\\) from a logistic distribution centred on \\(y_{prob}\\), plus an additional intercept term \\(u_j\\) for the group effect, which stretches out \\(y^*\\):\n\\[ y_{prob}=X\\beta \\\\ y^* \\sim \\text{logistic}(y_{prob}, 1)+u_j \\\\ y_{obs}=y^*\\geq0\\;?\\; 1 : 0 \\]\nThus, the population effects are set to contribute a variance of 3.29, while any group effects are added on top of this variance. If we subtract 3.29 from the total residual variance, we can find the variance attributable to the group effect, and take this as a proportion of the total variance.\nSimulation\rset.seed(1)\rHere, I simulate a dataset by way of the latent variable formulation above. This allows us to know the true proportion of variance explained by the group effect.\nSetup:\n\rBinary outcome yobs\rOne continuous predictor x1\rOne binary predictor x2\rA group level effect with varying intercepts uj.\r\rTo make it interesting, the distribution of the binary predictor x2 will vary by group, with some groups having many more observations with x2=1 and some having very few.\nmultibin \u0026lt;- function(n, J, beta1, beta2, jstd){\rgroup \u0026lt;- sample(J, n, replace=TRUE) #group membership indicator\rx1 \u0026lt;- rnorm(n,0,1) #standard continuous predictor variable\rx2 \u0026lt;- rep(NA,length(group))\rgroup_probs \u0026lt;- runif(20) #probability of x2=1 for members of each group\rfor (i in 1:length(group)){\rx2[i] \u0026lt;- sample(c(0,1), prob = c(1 - group_probs[group[i]], group_probs[group[i]]))\r}\ruj \u0026lt;- rnorm(J,0,jstd) #Group intercepts, normally distributed with standard deviation jstd\ryprob \u0026lt;- rep(NA,n)\rfor (i in 1:n){\ryprob[i] \u0026lt;- x1[i]*beta1 + x2[i]*beta2 #calculate yprob including only the population (fixed) effects.\r}\rystar \u0026lt;- rlogis(n,yprob,1) # var(ystar - yprob) = 3.29\rfor (i in 1:n){\rystar[i] \u0026lt;- ystar[i] + uj[group[i]] #Add the group effect to ystar\r}\ryobs \u0026lt;- ifelse(ystar \u0026gt;= 0, 1, 0) #Get the binary outcome using the threshold test\rreturn(data.frame(x1, x2, group, yprob, ystar, yobs))\r}\rNow a dataset is generated, note the value of 1.5 for jstd, the standard deviation of the group intercepts. This corresponds to a variance of 2.25.\nmdata \u0026lt;- multibin(n = 5000, J = 20, beta1 = 1.6, #3.2\rbeta2 = -0.7,\rjstd = 1.5)\rhead(mdata)\r## x1 x2 group yprob ystar yobs\r## 1 -1.8054836 0 6 -2.8887737 -4.0269975 0\r## 2 -0.6780407 0 8 -1.0848651 1.4070274 1\r## 3 -0.4733581 1 12 -1.4573729 -3.2942698 0\r## 4 1.0274171 0 19 1.6438673 -0.8470214 0\r## 5 -0.5973876 0 5 -0.9558201 -2.5958663 0\r## 6 1.1598494 1 18 1.1557590 2.8085406 1\rNow let’s plot the data, for a small subset of the groups (and a subsample within each group). Keep in mind that in a real study, ystar, being a latent variable, is invisible.\nmdata %\u0026gt;%\rfilter(group %in% seq(1,4)) %\u0026gt;%\rmutate(Group = paste0(\u0026quot;Group \u0026quot;, group)) %\u0026gt;%\rgroup_by(Group) %\u0026gt;%\rsample_n(100) %\u0026gt;%\rggplot()+\raes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+\rgeom_hline(yintercept = 0, col = \u0026quot;grey\u0026quot;, linetype = \u0026quot;dashed\u0026quot;)+\rgeom_point(alpha = 0.3)+\rgeom_point(aes(x = x1, y = yobs), size = 1.5)+\rscale_colour_discrete(guide = FALSE)+\rscale_shape_discrete(name = \u0026quot;x2\u0026quot;)+\rtheme_bw(14)+\rfacet_wrap(~Group)\rThe variance of the residuals should be approximately 3.29 + some variance attributable to the group-level effect. We can calculate the true variance from our simulation.\n\\[\\sigma^{2}_{\\text{total}}=\\text{var}(y^*-y_{prob})\\]\nvar(mdata$ystar - mdata$yprob)\r## [1] 6.064065\rHow much variance did the group level effect add?\n\\[\\sigma^{2}_{\\text{total}} = \\sigma^{2}_{e*} + \\sigma^{2}_{u}\\]\nWhere \\(\\sigma^{2}_{e*}\\) is the variance attributable to the population (fixed) effects and \\(\\sigma^{2}_{u}\\) is the variance attributable to the group (random) effects.\nvar(mdata$ystar - mdata$yprob) - (pi*pi)/3\r## [1] 2.774197\rWhat proportion of the total variance does this represent? I.e. what is the variance partition coefficient (VPC)?\n\\[ VPC = \\frac{\\sigma^{2}_{u}}{\\sigma^{2}_{e*}+\\sigma^{2}_{u}}\\]\n(var(mdata$ystar - mdata$yprob) - (pi*pi)/3)/var(mdata$ystar - mdata$yprob)\r## [1] 0.4574814\rAbout 46%.\nNow that we know the true variance values to expect from the simulation, we can fit a multilevel logistic regression model and see how close it gets. Here, I use the function glmer from the package lme4, which uses a maximum likelihood estimator. One point Prof Tranmer made in his talk was that Bayesian Markov Chain Monte Carlo estimators may do a better job for this particular problem, however, I have simulated a lot of observations so it shouldn’t really make a difference. If I wanted to use MCMC, the package brms can accept a formula in the same syntax as lme4.\nmmod1 \u0026lt;- glmer(yobs ~ x1 + x2 + (1 | group), data = mdata, family = \u0026quot;binomial\u0026quot;)\rsummary(mmod1)\r## Generalized linear mixed model fit by maximum likelihood (Laplace\r## Approximation) [glmerMod]\r## Family: binomial ( logit )\r## Formula: yobs ~ x1 + x2 + (1 | group)\r## Data: mdata\r## ## AIC BIC logLik deviance df.resid ## 4264.3 4290.4 -2128.2 4256.3 4996 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -6.5275 -0.4905 -0.1512 0.4567 13.0062 ## ## Random effects:\r## Groups Name Variance Std.Dev.\r## group (Intercept) 2.685 1.638 ## Number of obs: 5000, groups: group, 20\r## ## Fixed effects:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.14883 0.37175 -0.400 0.689 ## x1 1.60081 0.05353 29.905 \u0026lt; 2e-16 ***\r## x2 -0.75155 0.09457 -7.947 1.91e-15 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Correlation of Fixed Effects:\r## (Intr) x1 ## x1 -0.008 ## x2 -0.129 -0.120\rLooking at the summary, the variance attributable to the group (random effect) is pretty close to the true value. If this were a real model, the VPC of 46% would mean we would be certain to include the group effect. Now we can plot the varying interecepts model for \\(y^*\\) (for the same subset of data as above). Each group has two lines, one for each value of the predictor x2.\nmdata$predicted \u0026lt;- predict(mmod1, mdata)\rmdata %\u0026gt;%\rfilter(group %in% seq(1,4)) %\u0026gt;%\rmutate(Group = paste0(\u0026quot;Group \u0026quot;, group)) %\u0026gt;%\rgroup_by(Group) %\u0026gt;%\rsample_n(100) %\u0026gt;%\rggplot()+\raes(x = x1, y = ystar, col = factor(Group), shape = factor(x2))+\rgeom_hline(yintercept = 0, col = \u0026quot;grey\u0026quot;, linetype = \u0026quot;dashed\u0026quot;)+\rgeom_point(alpha = 0.3)+\rgeom_point(aes(x = x1, y = yobs), size = 1.5)+\rgeom_line(aes(x = x1, y = predicted, group = interaction(x2,group)))+\rscale_colour_discrete(guide = FALSE)+\rscale_shape_discrete(name = \u0026quot;x2\u0026quot;)+\rscale_x_continuous(limits = c(-2,2))+\rtheme_bw(14)+\rfacet_wrap(~Group)\r\rReference\rGoldstein H, Browne W, Rasbash J. Partitioning variation in generalised linear multilevel models. Understanding Statistics 2002; 1:223–232\n\r","date":1543190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543190400,"objectID":"4d9316349c7b034b4b53a81208e9ef77","permalink":"/post/multilevel-logistic-regression/","publishdate":"2018-11-26T00:00:00Z","relpermalink":"/post/multilevel-logistic-regression/","section":"post","summary":"I recently attended a whole-day workshop on multilevel modelling organised by NIASRA and taught by Professor Mark Tranmer, who is an expert in the field from the University of Glasgow.\nOne part of the workshop that I realised I didn’t have any familiarity with was the idea of how variance works in logistic regression, and in particular in a multilevel logistic regression.\nHere is one way of looking at logistic regression, in a formulation that reflects how a glm works:","tags":["multilevel modelling","logistic regression","variance"],"title":"Multilevel logistic regression (again)","type":"post"},{"authors":null,"categories":[],"content":"This one is just a snippet. I was reading through the Stan manual (version 2.17.0), and it has some nice examples of Stan models in chapter III, however, only the Stan model code is given for these. I decided to make a data simulation to try one out, specifically, the model in section 9.9: Hierarchical Logistic Regression.\nThe Stan code from the manual looks like this (after substituting the suggested improvements on pages 140-141).\ndata { int\u0026lt;lower=1\u0026gt; D; int\u0026lt;lower=0\u0026gt; N; int\u0026lt;lower=1\u0026gt; L; int\u0026lt;lower=0,upper=1\u0026gt; y[N]; int\u0026lt;lower=1,upper=L\u0026gt; ll[N]; row_vector[D] x[N]; } parameters { real mu[D]; real\u0026lt;lower=0\u0026gt; sigma[D]; vector[D] beta[L]; } model { mu ~ normal(0, 100); for (l in 1:L) beta[l] ~ normal(mu, sigma); for (n in 1:N) y[n] ~ bernoulli_logit(x[n] * beta[ll[n]]); }  I came up with the following R code to simulate some example data. I have annotated what each variable represents. To make it interesting, the widths of the population distributions from which the coefficients in beta have been drawn range from narrow to wide.\nset.seed(42) #number of predictors D \u0026lt;- 10 #number of observations N \u0026lt;- 10000 #matrix of predictors for each observation x \u0026lt;- matrix(rnorm(N*D,0,1), nrow = N, ncol = D) #number of categorical levels L \u0026lt;- 15 #Vector indicating level membership for each observation ll \u0026lt;- sample(seq(1,L), N, replace = TRUE) #centre of distribution for each predictor, #from which individual predictors are drawn for each level mu \u0026lt;- rnorm(D, 0, 1) #the sigma values progressively increase for each predictor in D. sigma \u0026lt;- seq(0.2,2,length.out = D) #Matrix of coefficients for each predictor in each level. beta \u0026lt;- matrix(NA, nrow = L, ncol = D) for (d in seq(1:D)){ beta[,d] \u0026lt;- rnorm(L, mu[d], sigma[d]) } #vector of ypred, in linear scale, prior to noise being added. ypred_linear \u0026lt;- rep(NA, N) for (n in seq(1:N)){ ypred_linear[n] \u0026lt;- x[n,] %*% beta[ll[n],] } #ypred in logistic scale invlogit \u0026lt;- function(x){exp(x)/(1+exp(x))} ypred \u0026lt;- invlogit(ypred_linear) #Bernoulli trial outcomes - these are our observed data y \u0026lt;- rbinom(N, 1, ypred)  The model is run by executing the following code. It is interesting to play around with the number of iterations. For a relatively simple regression model like this, a huge number of iterations is not necessary. It is also interesting to play around with the number of predictors D, levels L and observations N.\nlibrary(\u0026quot;rstan\u0026quot;) fit \u0026lt;- stan(\u0026quot;hierarchical_logistic.stan\u0026quot;, data = list(D = D, N = N, x = x, L = L, y = y, ll = ll), chains = 4, cores = 4, iter = 1000, warmup = 200, control = list(adapt_delta = 0.99))  As the widths of the underlying distributions from which the coefficients beta were drawn increase (sequentially, from first to last), so to does the uncertainty surrounding the estimated centre positions mu, and the estimates of the widths, sigma.\nHere are the estimated centre positions of these underlying distributions.\nplot(fit, pars = \u0026quot;mu\u0026quot;)  And here are the estimates of the corresponding widths.\nplot(fit, pars = \u0026quot;sigma\u0026quot;)  Because there are 150 indivudal coefficients in beta, I won\u0026rsquo;t plot them all. Here, I will plot the estimates across all levels for the first and last coefficients.\nplot(fit, pars = paste0(\u0026quot;beta[\u0026quot;, seq(1:L), \u0026quot;,1]\u0026quot;))  plot(fit, pars = paste0(\u0026quot;beta[\u0026quot;, seq(1:L), \u0026quot;,\u0026quot;, D, \u0026quot;]\u0026quot;))  Note the wider distribution of beta[,10] coefficients, arising from the greater size of sigma[10]. Also note that the credibility intervals on the more extreme values in beta[,10] are wider.\n","date":1540339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540339200,"objectID":"83e3a0fac55dad0130e0af8a737f94ac","permalink":"/post/hierarchical-logistic-regression/","publishdate":"2018-10-24T00:00:00Z","relpermalink":"/post/hierarchical-logistic-regression/","section":"post","summary":"This one is just a snippet. I was reading through the Stan manual (version 2.17.0), and it has some nice examples of Stan models in chapter III, however, only the Stan model code is given for these. I decided to make a data simulation to try one out, specifically, the model in section 9.9: Hierarchical Logistic Regression.\nThe Stan code from the manual looks like this (after substituting the suggested improvements on pages 140-141).","tags":["stan","logistic regression"],"title":"Hierarchical Logistic Regression","type":"post"},{"authors":null,"categories":null,"content":"In this post, I will run through a simple simulation using only base R, and demonstrate a way to do power analysis by using this simulation. Say we have a cohort of 500 first-year students commencing their second session of study. We decide to offer a special tutoring service to some of the students who did not go well in their first session. To do this, we arrange students in ascending order of their averaged first session final marks, and go down the list calling students and offering tutoring services until 50 students have taken up the offer. At the end of the second session, we want to see if this made a difference. Here we will consider a simple situation where all relevant variables have been taken into account, and the effect is therefore consistent for all students - if only real life were so simple!\nAlthough I am usually a devotee of the tidyverse, to keep things simple and focus on the procedure, I have used only base R in this post.\nSimulation\rStart with the basic parameters and a latent variable we’ll call aptitude, which for simplicity’s sake will be the only thing (besides a bit of luck) that determines students’ marks. As a latent variable, the most appropriate scale for this is a standard normal, i.e. a normal distribution centred at zero with a standard deviation of one.\nset.seed(42)\rn_students \u0026lt;- 500\rn_tutoring \u0026lt;- 50\rmeanmark \u0026lt;- 60.0\rmarks_sd \u0026lt;- 12.0\raptitude \u0026lt;- rnorm(n_students, 0.0, 1.0)\rstudents \u0026lt;- data.frame(aptitude)\rGenerate a reasonable looking distribution of first-session marks, based on students’ aptitude, plus or minus a little bit of luck.\nluck \u0026lt;- 2\rstudents$first_session_marks \u0026lt;- rnorm(n_students, students$aptitude*marks_sd + meanmark, luck)\r#It is possible for some values to fall off the scale.\rstudents$first_session_marks[students$first_session_marks \u0026gt; 100] \u0026lt;- 100\rstudents$first_session_marks[students$first_session_marks \u0026lt; 0] \u0026lt;- 0\rhist(students$first_session_marks, breaks = 15)\rTutoring offer\rNow we start calling students to offer them tutoring, starting from those with the lowest first session marks. Here, the probability of a student accepting tutoring is 0.8 - first_session_mark/100, i.e. students with lower marks are more likely to accept tutoring.\nstudents \u0026lt;- students[order(students$first_session_marks),]\rn_tutoring_places_accepted \u0026lt;- 0\rtutored \u0026lt;- rep(0, n_students)\ri \u0026lt;- 1\rwhile (n_tutoring_places_accepted \u0026lt; n_tutoring \u0026amp; i \u0026lt; n_students) {\rrval \u0026lt;- runif(1) if (rval \u0026lt; 0.8 - students$first_session_marks[i]/100){\rtutored[[i]] \u0026lt;- 1\rn_tutoring_places_accepted \u0026lt;- n_tutoring_places_accepted + 1\r}\ri \u0026lt;- i + 1\r}\rstudents$tutored \u0026lt;- tutored\rsum(tutored)\r## [1] 50\r\rOutcomes\rNow simulate second session marks. These will also be based on students’ underlying aptitude, and will be similar to the first session marks, but because the courses have changed there may be a different overall mean and scaling of marks with respect to to aptitude. To keep things simple (and unrealistic), students’ underlying aptitude has not changed. In this model, there is an intercept term that gives the average (untutored) mark, a slope term that scales the mark with respect to aptitude, and another interept term for the effect associated with tutoring. Realised values of the second session mark are then drawn from Gaussian distributions centred at these calculated mark values, with a defined standard deviation that is the same for all students. Thus, for student \\(i\\) we have:\n\\[\\text{Second session mark}_i \\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i = \\beta_0 + \\beta_1\\text{aptitude}_i + \\beta_2\\text{Tutored}_i \\] Define the parameters, and generate the second session marks according to the model. Here, we have very ambitiously assumed that tutoring increases students marks (to say nothing of whether it changes their aptitude) by 3 points.\nbeta_0 \u0026lt;- 58 #This is the mean mark in second session for untutored students\rbeta_1 \u0026lt;- 0.9\rbeta_2 \u0026lt;- 3\rsigma \u0026lt;- 2 #Luck in second session.\rstudents$second_session_marks.mu \u0026lt;- beta_0 + beta_1*students$aptitude*marks_sd + beta_2*students$tutored\rstudents$second_session_marks \u0026lt;- rnorm(n_students, students$second_session_marks.mu, sigma)\r#Once again, it is possible that some values could have fallen off the scale, and this is one simple way to fix that:\rstudents$second_session_marks[students$second_session_marks \u0026gt; 100] \u0026lt;- 100\rstudents$second_session_marks[students$second_session_marks \u0026lt; 0] \u0026lt;- 0\rhist(students$second_session_marks, breaks = 15)\rPlot the first and second session marks, coloured by whether tutoring was accepted.\nplot(students$first_session_marks, students$second_session_marks, col = factor(students$tutored, levels = c(0,1)))\r\rPropensity scores\rStudents who received tutoring have had a small but appreciable bump in their marks compared with similar students who did not receive tutoring. To make this comparison, we should limit the range students included in the analysis. One way this is often done for observational studies such as this is by propensity scores. These quantify the probability of a study subject choosing to receive the treatment, based on a range of relevant variables. In this case, the only variable we have at hand is first session marks, but typically we would also include age, sex, etc. To get the propensity score, we do a logistic regression of ‘tutored’ on first session marks.\npropmod \u0026lt;- glm(tutored ~ first_session_marks, family = \u0026quot;binomial\u0026quot;, data = students)\rsummary(propmod)\r## ## Call:\r## glm(formula = tutored ~ first_session_marks, family = \u0026quot;binomial\u0026quot;, ## data = students)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1293 -0.3683 -0.2233 -0.1008 2.2517 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 6.30506 1.01662 6.202 5.58e-10 ***\r## first_session_marks -0.16147 0.02074 -7.785 6.96e-15 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 325.08 on 499 degrees of freedom\r## Residual deviance: 225.80 on 498 degrees of freedom\r## AIC: 229.8\r## ## Number of Fisher Scoring iterations: 6\rAs expected, lower first session marks equates to a greater probability of receiving tutoring. What was the lowest propensity score for a student who received tutoring?\nstudents$propensity \u0026lt;- predict(propmod, newdata = students, type = \u0026quot;response\u0026quot;)\r(lowestpropensity \u0026lt;- min(students$propensity[students$tutored==1]))\r## [1] 0.07925736\rOne approach is to do propensity score matching, in which each study subject who received the treatment is matched with one (or more) subjects with an equal (or very similar) propensity score. Another approach is to simply use the propensity scores to define the range of study subjects to be included. Here, we will take the latter approach and include all non-tutored students with a propensity score of 0.079 or greater in the ‘control’ group. (Of course, given that the only predictor of propensity is first session mark, we could simply use that as the cut off, but here I want to demonstrate the basic process).\nstudents.subset \u0026lt;- students[students$propensity \u0026gt;= lowestpropensity,]\rplot(students.subset$first_session_marks, students.subset$second_session_marks, col = factor(students.subset$tutored, levels = c(0,1)))\r\rMeasuring the tutoring effect\rTo measure the bump in marks, fit a linear regression, with a term for the tutoring effect. Students who were tutored have a value of 1 for the variable ‘tutored’, while those who weren’t tutored have a value of 0, so any coefficient on this variable is essentially an intercept term.\n\\[\\text{Second session mark}_i \\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i = \\beta_0 + \\beta_1\\text{first_session_mark}_i + \\beta_2\\text{Tutored}_i \\]\nregression_model \u0026lt;- lm(second_session_marks ~ first_session_marks + tutored, data = students.subset)\rsummary(regression_model)\r## ## Call:\r## lm(formula = second_session_marks ~ first_session_marks + tutored, ## data = students.subset)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -9.7814 -1.8846 0.1392 2.0084 6.3410 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.69305 1.73438 1.553 0.123 ## first_session_marks 0.93452 0.03659 25.543 \u0026lt; 2e-16 ***\r## tutored 3.21221 0.47583 6.751 2.96e-10 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 2.735 on 151 degrees of freedom\r## Multiple R-squared: 0.8142, Adjusted R-squared: 0.8117 ## F-statistic: 330.9 on 2 and 151 DF, p-value: \u0026lt; 2.2e-16\r Note that we are now regressing against first session marks as a measure of the underlying aptitude from which the marks were drawn. Nonetheless, the model has retrieved fairly reasonable estimates for the coefficients - no surprise, they are a less noisy than real data, all relevant variables have been included, and the effect size is appreciable and consistent.\nBecause we can see that the coefficient on ‘tutored’ is non-zero with p \u0026lt; 0.05, we can also do an analysis of variance to see whether the reduction in the residual sum of squares caused by including the tutored variable is significant. For the purpose of assessing whether tutoring has an effect, this is not the statistic of interest. It can be useful, however, in situations like when we want to make a predictive model, and include all the variables that will have a noticeable effect.\nanova(regression_model)\r## Analysis of Variance Table\r## ## Response: second_session_marks\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## first_session_marks 1 4609.8 4609.8 616.157 \u0026lt; 2.2e-16 ***\r## tutored 1 341.0 341.0 45.573 2.964e-10 ***\r## Residuals 151 1129.7 7.5 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rIndeed, there is a significant reduction in the RSS by including ‘tutored’.\n An interesting point to note here is that some of the students who were offered tutoring may have received that offer because they were unlucky in first session and got marks a bit lower than what would be expected for their aptitude. In the second session, these students could be expected to get better marks regardless of being tutored - this is called regression to the mean, and could be very problematic for a study such as this one.\n \r\rPower analysis by simulation\rA good idea before doing a study like this is to calculate its power - the probability of rejecting the null hypothesis when the null hypothesis is false (at a given significance level). This calculation can either help us to decide how many study subjects are needed to reach a desired power level, or if we are unable to increase the number of study subjects it can at least give us a realistic appreciation for the power of our study. There is a nice illustration of the concept of power in this post by Kristoffer Magnusson.\nWe can use this simulation to find the power for an estimated effect size, or to find the minimum effect size needed to achieve a given power. By replicating the above simulation thousands of times, we can quantify the fraction for which the p-value of the tutoring effect coefficient was below the significance level. It is important to note that the effect size here is measured in units of marks, whereas effect sizes are often put into standard units by dividing by the standard deviation. The power simulation uses a simpler model to decide which students receive tutoring - for the bottom 100 students, a random 50 % accept tutoring. This mimics a situation in which we have no model for who is more likely to accept tutoring. The results of this power analysis could therefore mislead us.\npower_simulation \u0026lt;- function(n_tutored = 50, n_untutored = 450, tutoring_effect, sigma_secondsession, beta0 = 58, beta1 = 0.9, marks_sd = 12, method = \u0026quot;pval\u0026quot;){\r#Assign aptitude scores and generate first session marks\rnstudents \u0026lt;- n_tutored + n_untutored aptitude \u0026lt;- rnorm(nstudents, 0, 1)\rstudents \u0026lt;- data.frame(aptitude)\rstudents$first_session \u0026lt;- rnorm(nstudents, students$aptitude*marks_sd + 60, 2)\rstudents \u0026lt;- students[order(students$first_session),]\r#Randomly assign tutoring to 50% of students in the bottom n_tutored*2 of first session marks.\rtutored.indices \u0026lt;- sample(seq(1,n_tutored*2), n_tutored)\rtutored \u0026lt;- rep(0, nstudents)\rtutored[tutored.indices] \u0026lt;- 1\rstudents$tutored \u0026lt;- tutored\r#Generate second session marks from aptitude, with a bump in marks for those who were tutored\rstudents$second_session \u0026lt;- rnorm(nstudents, beta0 + beta1*students$aptitude*marks_sd + tutoring_effect*students$tutored, sigma_secondsession)\r#Subset to include only students in the range where tutoring was offered\rmaxtutoredmark \u0026lt;- max(students$first_session[students$tutored==1])\rstudents.subset \u0026lt;- students[students$first_session \u0026lt;= maxtutoredmark,]\r#Fit a linear regression to get the p-value on the tutoring effect estimate.\rlmod \u0026lt;- summary(lm(second_session ~ first_session + tutored, data = students.subset))\rif (method == \u0026quot;pval\u0026quot;){return(lmod$coefficients[3,4]) #The pvalue for the tutoring_effect estimate.\r#An option to return the coefficient value only if it is significant.\r} else if (method == \u0026quot;coeff\u0026quot; \u0026amp; lmod$coefficients[3,4] \u0026lt; 0.05){return(lmod$coefficients[3,1])\r} else if (method == \u0026quot;coeff\u0026quot; \u0026amp; lmod$coefficients[3,4] \u0026gt;= 0.05){return(NA)}\r}\rpvals \u0026lt;- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 1, sigma_secondsession = 2))\rmean(pvals \u0026lt; 0.05)\r## [1] 0.4513\rSo, we have a calculated power of 0.5 when there is an effect of +1 mark. The histogram below shows values of the coefficent for all simulations in which the p-value was below 0.05. Surprisingly, the true value of 1 is at the lower end of the range here. Selecting for only the statistically significant values has selected for those with inflated estimates, and the regression to the mean problem mentioned earlier is probably also making a contribution.\ntutoring_effects \u0026lt;- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 1, sigma_secondsession = 2, method = \u0026quot;coeff\u0026quot;))\rhist(tutoring_effects)\rIn a noisier simulation, or one with a smaller effect size, the tutoring estimate can even come out as both negative and statistically significant. Because we don’t have the power to detect this effect, only the measured effects with a large absolute value turn out to be statistically significant. If we did a study under these conditions we would end up with one of the following situations:\n\rNo significant effect (type II error)\rSignificant effect of inflated magnitude and correct sign (type M error).\rSignificant effect of inflated magnitude and incorrect sign (type M and type S error).\r\rsmall_noisy_tutoring_effects \u0026lt;- replicate(10000, power_simulation(n_tutored = 50, n_untutored = 450, tutoring_effect = 0.1, sigma_secondsession = 6, method = \u0026quot;coeff\u0026quot;))\rhist(small_noisy_tutoring_effects, breaks = 50)\r Concluding remarks\rGoing into a power analysis it is common to have a personal bias to thinking our effect size will be greater than it really is, and that the data are less noisy than they really are. Without going too much into the whys and wherefores of power analysis and null hypothesis significance testing, it is good practice (pdf) to use a conservative estimate of the effect size based on external information (e.g. prior studies). If we wanted to make a proper attempt at doing a power analysis for an experiment (observational study) like the one described above, we would need to have reasonable estimates for the distributions of marks, and a measure of how well correlated these are for students from session-to-session, to figure out the relationship to the underlying aptitude. If we go a step further and take into account that individual students’ aptitudes can be on an upward or downard trajectory, it starts to get very complicated.\n \r\r","date":1536796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536796800,"objectID":"d9cb7f2c3cc82abc9d3e37eb34d71942","permalink":"/post/outcomespost/","publishdate":"2018-09-13T00:00:00Z","relpermalink":"/post/outcomespost/","section":"post","summary":"In this post, I will run through a simple simulation using only base R, and demonstrate a way to do power analysis by using this simulation. Say we have a cohort of 500 first-year students commencing their second session of study. We decide to offer a special tutoring service to some of the students who did not go well in their first session. To do this, we arrange students in ascending order of their averaged first session final marks, and go down the list calling students and offering tutoring services until 50 students have taken up the offer.","tags":["regression","statistics","power analysis"],"title":"Quantifying an educational effect","type":"post"},{"authors":null,"categories":null,"content":"\rIn this post, I simulate an example experiment in which there is a non-linear relationship with underlying parameters that vary by group membership, before fitting the data using Bayesian regression in Stan. In this example, inspired by a real-world problem (with details changed to protect future Nobel prize winners, so try not to read too much into it), several mutant strains of a species of bacteria have been exposed to a range of concentrations of Chemical X. Individual bacteria are exposed, and their response is to either turn green (success) or not. Intriguingly, if a lot of trials are measured of a range of concentrations for a single strain (mutant A, below), there is a bell curve-shaped rise and fall in successes as a function of concentration. Resources are insufficient to measure this many trials for each mutant, so 20 trials at each concentration for each mutant instead. However, some of these trials failed due to a technical glitch and the number of trials ranges from 10-20. Of particular interest is position of the centre of each strain’s Gaussian-shaped response curve.\nFor this problem, I will take the approach of fitting a model to the data that is based on an understanding of how the data were generated. This will be done using Bayesian inference, which goes a step further than simply finding a single set of most likely parameters, by finding the probability distribution for a wide range of possibile underlying paramters. For this, I use the probabilistic programming language Stan.\nModel\rFor each mutant, at each concentration, there is a certain number of trials and a certain number of successes. If the experiment were to be repeated many times, would we expect to get the exact same data every time? Of course not, but the numbers should be similar. The number of successes that are observed in any given experiment is stochastically drawn from some distribution. Our first task in defining the model is to identify the most appropriate distribution. In this case, the distribution that makes the most sense is the binomial distribution, which has two parameters: the number of trials, and the probability of success in each trial.\nFor each mutant at each concentration: the observed number of successes \\(Y\\) is drawn from a binomial distribution parameterised by the probability of success \\(p\\) and number of trials. The ~ symbol denotes a stochastic relationship.\n\\[Y \\sim \\text{Binomial(trials}\\text{, } p) \\]\nIn the second part of the model, the true success probability \\(p\\) for mutant \\(j\\) at concentration \\(x_i\\) is given by a Gaussian curve parameterised by the peak height \\(h_j\\) (in units of success probability), peak centre position \\(c_j\\) (in units of concentration), and its width (standard deviation) \\(\\sigma\\) (in units of concentration).\n\\[p_{j,i} = h_j \\text{ exp}{\\frac{-(x_i-c_j)^2}{2\\sigma_j^2}}\\]\nBayes’ Rule\rFor a given set of parameters, a range of observations can be obtained stochastically from the binomial distribution. The corollary of this is that for a given set of observations, a range of underlying parameters could have created them. The above model can be used to calculate the likelihood of the observed data for any of a range of candidate parameter values. From this, the probability of any particular set of parameters having created the data can also be assessed. This is achieved using Bayes’ rule, given here in the un-normalised form:\n\\[P(\\text{Parameters | Data}) \\propto P(\\text{Data | Parameters}).P(\\text{Parameters}) \\] Where:\n\r\\(P(\\text{Parameters | Data})\\) is the posterior probability - the probability of a given set of candidate parameters conditional on the observed data.\r\\(P(\\text{Data | Parameters})\\) is the likelihood. The above model is critical in calculating this - for a given set of parameters, how likely are the observed data to have been stochastically drawn from the distribution these parameters imply?\r\\(P(\\text{Parameters})\\) is the prior probability of the candidate parameters. This may have support from previous experiments, or it may have to represent an educated guess.\r\rIn the normalised form, the terms on the right are divided by the marginal likelihood of observing the data, integrated over all possible parameters, to make the terms on the right equal to the posterior probability on the left. In general, this is more difficult to achieve, and if our only aim is to assess the probability of one set of paramters relative to another, it is not necessary.\n\rSampling\rHow can Bayes’ rule be applied to the above problem? For relatively simple problems, Bayes’ rule can be applied analytically, and an exact solution for the posterior probability distribution can be found. For more complicated problems like this one, a sampling approach is the only viable option. These methods build up a picture of what the posterior probability distribution looks like, one set of candidate parameters at a time. The challenge in sampling is to efficiently explore the posterior probability distribution.\nThere are several Markov Chain Monte Carlo (MCMC) sampling algorithms, including Gibbs sampling and Hamiltonian Monte Carlo, but the basic principles of these are pretty much as follows:\nSetup\n\rA prior probability distribution is specified for each parameter.\rA formula for the likelihood is specified.\rA random (or specified) set of parameters are taken as the initial candidate parameters\rThe number of sampling iterations is defined. Depending on the problem, the first several hundred or thousand of these will be discarded, as they have started at some random part of parameter space and have not yet equilibrated to the posterior distribution.\r\rSampling\nThe likelihood of obtaining the observed data given the candidate parameters is calculated and multiplied by the prior probability. This gives the value on the right hand side of Bayes’ (un-normalised) formula.\rThe candidate set of parameters are either accepted or rejected, in some manner that is dependent on their probability. For example, by taking it as a ratio over the last accepted probability, and then stochastically choosing to accept or reject based on the value of this ratio.\rA new set of candidate parameters is generated by treating the last accepted set of parameters as a jumping off point. For example, a set of random numbers may be added to these.\r\rIn this way, the sampler explores each part of parameter space in proportion to its probability, and ultimately should thoroughly explore the joint probability density of all the parameters. Note that probabilities are calculated as log probabilities - this way, they can be added rather than multiplied, which is computationally much more achievable.\nHere’s a really great blog post by Thomas Wiecki that explains the whole process very nicely.\n\r\rSimulation\rA great way to proceed is by simulating data based on this model, and then fitting it with the corresponding regression model. In this way, we gain a deeper appreciation for how the model works, and can much more readily spot flaws in our thinking.\n# Give each mutant a name.\rn_mutants \u0026lt;- 6\rmutants \u0026lt;- LETTERS[1:n_mutants] # Population-level intercepts\rpop_height_logodds \u0026lt;- 0\rpop_centre \u0026lt;- 70\rpop_width \u0026lt;- 1.5 #Population scaling parameters\rgroup_scale_height_logodds \u0026lt;- 1\rgroup_scale_centre \u0026lt;- 4\rgroup_scale_width \u0026lt;- 0.5\r Random parameter draw\rTrue height and centre parameters for each mutant are now drawn from Gaussian distributions with the population centres and group standard deviations as given above. A random seed is specified to make this reproducible. The height values are in log-odds units, and are transformed to probability using the invlogit function. We can now make a table of the key parameters for each mutant. Note that mutants E and F have the same centre, while mutants B, C and E have the same height value.\n#invlogit function for converting log odds to probability\rinvlogit \u0026lt;- function(x){exp(x)/(1+exp(x))}\r#set random seed for reproducibility\rset.seed(100)\rheight_raw_logodds \u0026lt;- rnorm(n = n_mutants, mean = 0, sd = 1)\rcentre_raw \u0026lt;- rnorm(n = n_mutants, mean = 0, sd = 1)\rwidth_raw \u0026lt;- rnorm(n = n_mutants, mean = 0, sd = 1)\rmutant_height_logodds \u0026lt;- pop_height_logodds + group_scale_height_logodds * height_raw_logodds\rmutant_centre \u0026lt;- pop_centre + group_scale_centre * centre_raw\rmutant_width \u0026lt;- pop_width + group_scale_width * width_raw\r# Transform these parameters from log-odds to probability\rmutant_height_p \u0026lt;- invlogit(mutant_height_logodds)\r# Data summary\rmutant.frame \u0026lt;- data.frame(\u0026quot;mutant\u0026quot; = mutants, \u0026quot;height_logodds\u0026quot; = mutant_height_logodds, \u0026quot;height_p\u0026quot; = mutant_height_p,\r\u0026quot;centre\u0026quot; = mutant_centre, \u0026quot;width\u0026quot; = mutant_width)\rkable(mutant.frame, format = \u0026quot;html\u0026quot;, digits = 1)%\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;))\r\r\rmutant\r\rheight_logodds\r\rheight_p\r\rcentre\r\rwidth\r\r\r\r\r\rA\r\r-0.5\r\r0.4\r\r67.7\r\r1.4\r\r\r\rB\r\r0.1\r\r0.5\r\r72.9\r\r1.9\r\r\r\rC\r\r-0.1\r\r0.5\r\r66.7\r\r1.6\r\r\r\rD\r\r0.9\r\r0.7\r\r68.6\r\r1.5\r\r\r\rE\r\r0.1\r\r0.5\r\r70.4\r\r1.3\r\r\r\rF\r\r0.3\r\r0.6\r\r70.4\r\r1.8\r\r\r\r\r \rPredictor variable and number of trials\r# Predictor variable\rconc_range \u0026lt;- 10\rconc_interval \u0026lt;- 1\rconc \u0026lt;- seq(pop_centre-conc_range, pop_centre + conc_range, conc_interval)\r# Maximum number of repeats/trials\rmax_trials = 10\r#Probability of max trials\rprob_trials = 0.95 #A high value to simulate a situation where there is an intended number of trials, but some have failed for technical reasons.\r \rProbability of success\rThe Gaussian function is defined and a probability matrix describing the probability of success for each mutant at each tested concentration is calculated, given the parameters height, centre, and width for each mutant.\ngaussian \u0026lt;- function(height,centre,width,x){\rreturn(height * exp(-0.5* ( ((x - centre)^2)/(width^2)) ))\r}\rprobability.matrix \u0026lt;- matrix(data = NA, nrow = length(conc), ncol = n_mutants, dimnames = list(conc, mutants))\rfor (j in 1:n_mutants){\rfor (i in 1:length(conc)){\rprobability.matrix[i,j] \u0026lt;- gaussian(mutant_height_p[j], mutant_centre[j], mutant_width[j], conc[i])\r}\r}\r\rExperiment simulation\rFor each mutant, at each concentration, the number of trials can vary. Here this is achieved by drawing the number of trials from a binomial distribution (to simulate random technical failures), but that is not crucial to the model.\nAn experiment is now simulated by drawing from a binomial distribution for each mutant-concentration combination, parameterised the corresponding values from the probabilities matrix and number of trials matrix.\nn_trials.matrix \u0026lt;- matrix(data = rbinom(n = length(conc)*n_mutants, size = max_trials, prob = prob_trials), nrow = length(conc), ncol = n_mutants, dimnames = list(conc, mutants))\r# Simulate experiments - get the number of successes given the probability from the above matrix and the number of trials.\rsuccesses.matrix \u0026lt;- matrix(data = NA, nrow = length(conc), ncol = n_mutants, dimnames = list(conc, mutants))\rfor (j in 1:n_mutants){\rfor (i in 1:length(conc)){\rsuccesses.matrix[i,j] \u0026lt;- rbinom(1, prob = probability.matrix[i,j], size = n_trials.matrix[i,j])\r}\r}\r \rReshape data\rWe now make use of the tidyverse set of packages. Data are reformatted from a wide format in which each mutant has its own column to tidy format, which is easier to work with. In this long table format we have a single column to specify the mutant, and single columns for concentration, number of trials and number of successes. For convenience, we also tack on columns with the true mutant parameters (these values are simply repeated in each row for the same mutant). If you look at the ggplot function below, you will see the advantage that tidy format confers - we can now describe the plot for all mutants at once, rather than having to add layers for each mutant.\nlibrary(reshape2)\r# Use the melt function from reshape2, this is similar to tidyr\u0026#39;s gather, but it also works on matrices and arrays.\r# This converts data from wide to long format.\rn_trials.frame \u0026lt;- melt(n_trials.matrix)\rcolnames(n_trials.frame) \u0026lt;- c(\u0026quot;conc\u0026quot;, \u0026quot;mutant\u0026quot;, \u0026quot;n_trials\u0026quot;)\rprob.frame \u0026lt;- melt(probability.matrix)\rcolnames(prob.frame) \u0026lt;- c(\u0026quot;conc\u0026quot;, \u0026quot;mutant\u0026quot;, \u0026quot;probability\u0026quot;)\rsuccesses.frame \u0026lt;- melt(successes.matrix)\rcolnames(successes.frame) \u0026lt;- c(\u0026quot;conc\u0026quot;, \u0026quot;mutant\u0026quot;, \u0026quot;successes\u0026quot;)\rdetach(package:reshape2)\r# Combine\rexperiment_data \u0026lt;- left_join(successes.frame, n_trials.frame) %\u0026gt;%\rleft_join(prob.frame) %\u0026gt;%\rleft_join(mutant.frame)\r#Trim away the zero readings.\rexperiment_data \u0026lt;- experiment_data %\u0026gt;%\rfilter(conc \u0026gt; centre - 4*width \u0026amp; conc \u0026lt; centre + 4*width)\r And this is how our simulated measurements look:\n# Plot\rggplot(experiment_data)+\raes(x = conc, y = successes/n_trials, group = mutant)+\rgeom_vline(data = mutant.frame, aes(xintercept = centre), linetype = \u0026quot;dashed\u0026quot;)+\rgeom_line(aes(y = n_trials/max_trials), linetype = \u0026quot;dashed\u0026quot;, colour = \u0026quot;grey\u0026quot;)+\rgeom_line(aes(y = probability), colour = \u0026quot;blue\u0026quot;)+\rgeom_point(size = 1)+\rtheme_bw(14)+\rfacet_wrap(~mutant,ncol=2)+\rlabs(x = \u0026quot;Concentration\u0026quot;, caption = paste0(\u0026quot;Horizontal dashed line represents number of trials as fraction of maximum trials (\u0026quot;, max_trials, \u0026quot;).\\nVertical dashed line shows the true centre.\\nBlue curve shows the true probability.\u0026quot;))\r  \r\rStan model\rHere I use the tidy format data as the input. The mutants are named by an index rather than letter. All input data must be packaged together in a list.\nmlist \u0026lt;- data.frame(\u0026quot;mutant\u0026quot; = mutants, mutant_num = seq(1:n_mutants))\rexperiment_data \u0026lt;- experiment_data %\u0026gt;% left_join(mlist)\rdat_allmutants = list(\rJ = n_mutants,\rN = nrow(experiment_data),\rmutant = experiment_data$mutant_num,\rx = experiment_data$conc,\rn_trials = experiment_data$n_trials,\ry = experiment_data$successes\r)\r The stan model is stored in a separate .stan file, however, it is also possible to provide it as a string. It is split into blocks as follows, shown individually for convenience. See the Stan manual for more information.\nData block\rdata {\rint\u0026lt;lower=1\u0026gt; J; //number of mutants\rint\u0026lt;lower=1\u0026gt; N; //Number of observations\rint\u0026lt;lower=1,upper=J\u0026gt; mutant[N]; //mutant for observation n\rvector[N] x; //concentration for observation n\rint\u0026lt;lower=0\u0026gt; n_trials[N]; //number of trials for observation n\rint\u0026lt;lower=0\u0026gt; y[N]; //Number of survivors for observation n\r}\r\rValues in the data block should match what we have put in our data list. The choice of type for each variable in the data block is crucial, and the Stan manual is very helpful in this area.\n\rParameters block\rparameters {\rreal mu_centre;\rreal\u0026lt;lower=0\u0026gt; sigma_centre;\rvector[J] centre_raw;\rreal mu_heightlogodds;\rreal\u0026lt;lower=0\u0026gt; sigma_heightlogodds;\rvector[J] heightlogodds_raw;\rreal\u0026lt;lower=0\u0026gt; mu_width_squared;\rreal\u0026lt;lower=0\u0026gt; sigma_width_squared;\rvector[J] width_raw_squared;\r}\rThese are the parameters to be estimated by the regression fit. The parameters for each mutant are loosely treated as coming from common distributions. A population centre value is estimated, along with a scaling value. Individual mutant deviations are then drawn from centred unit normals. The mutant parameter is then given in the transformed parameters block as the mutant deviation times the scaling factor plus the population centre value. This is the non-centred parameterisation, which helps to avoid sampling problems when there are only a few members in the group. By contrast, in the centred parameterisation, individual mutant parameters would be drawn directly from a distribution with an estimated population centre and standard deviation.\nNote that because the width parameter only appears as its square in the gaussian formula, it is simplified to width_squared here.\n\rTransformed parameters block\rtransformed parameters {\rvector[J] height;\rvector[J] centre;\rvector[J] heightlogodds;\rvector\u0026lt;lower=0\u0026gt;[J] width_squared;\rcentre = mu_centre + sigma_centre*centre_raw;\rheightlogodds = mu_heightlogodds + sigma_heightlogodds * heightlogodds_raw;\rheight = inv_logit(heightlogodds);\rwidth_squared = mu_width_squared + sigma_width_squared * width_raw_squared;\r}\rHere, individual centre, height and width_squared values for each mutant are calculated as the population centre value plus the individual mutant’s deviation times the scaling factor.\n\rModel block\rmodel {\rvector[N] psurvive;\rfor (n in 1:N)\rpsurvive[n] = height[mutant[n]] * exp( -0.5* ( ((x[n] - centre[mutant[n]])^2) / (width_squared[mutant[n]]) ) );\rmu_centre ~ normal(65,20); sigma_centre ~ cauchy(0,10);\rcentre_raw ~ normal(0,1);\rmu_heightlogodds ~ normal(0.5,2);\rsigma_heightlogodds ~ cauchy(0,2);\rheightlogodds_raw ~ normal(0,1);\rmu_width_squared ~ cauchy(0,5);\rsigma_width_squared ~ cauchy(0,3);\rwidth_raw_squared ~ normal(0,1);\ry ~ binomial(n_trials, psurvive);\r}\rPrior probability distributions are specified for each parameter, as well as the likelihood. The likelihood is given by ‘y ~ binomial(n_trials, psurvive)’ in conjunction with the loop that gives psurvive values at each mutant/concentration combination.\n\rGenerated quantities block\rgenerated quantities {\rvector\u0026lt;lower=0\u0026gt;[J] width;\rwidth = sqrt(width_squared);\r}\rHere, the width is output as the square root of the width_squared parameter. This, along with anything else that we might put in this block, is output purely for convenience, and is not part of the fitting process.\n  \rRun the model\rIn this case, the model is very fast. The slowest parts are compiling the model, and finding initial values that do not conflict with the parameter restrictions (e.g. non-negative values of width_squared). But it still only takes about 20 seconds in total, including 4.5 seconds of actual sampling. If this had been too much of a problem, it is possible to specify initial values (different values must be supplied to each chain). In my experience, having two or three grouping variables and \\(10^4\\)-\\(10^5\\) observations means that the model could take hours or tens of hours to fit.\npost_warmup \u0026lt;- 2000\rwarmup_draws \u0026lt;- 1000\rn_chains \u0026lt;- 4\rfit \u0026lt;- stan(file = \u0026quot;mutant_model.stan\u0026quot;, model_name = \u0026quot;example\u0026quot;,\rdata = dat_allmutants, iter = post_warmup + warmup_draws, warmup = warmup_draws, chains = n_chains, cores = 4,\rverbose = FALSE)\r I am not going to devote the space here to investigating how well the model ran - convergence etc., but I will recommend that Shinystan is an excellent package for exploring these issues.\nlibrary(shinystan)\rlaunch_shinystan(fit)\r The mutant centre values have turned out quite reasonable.\nplot(fit, pars = \u0026quot;centre\u0026quot;)\rMost pairs of mutants are distinct, however, some are overlapping. We will need to delve further into the posterior draws to assess these.\n  \r\rPosterior draws\rExtract some example posterior draws (note that these start from the first post-warmup draw) and calculate the corresponding probability curves given by the parameters in each draw. To get nice looking curves we will sample concentrations at finer intervals than we did in our experiment. Note that the package Bayesplot can do all of this (including supplying a custom function of the parameters, such as the gaussian), but for the sake of learning I have gone through this manually.\nn_draws \u0026lt;- 200\rdraws \u0026lt;- paste0(\u0026quot;draw_\u0026quot;, 1:n_draws)\rlist_of_draws \u0026lt;- extract(fit, pars = c(\u0026quot;centre\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;width\u0026quot;))\rcentrevals \u0026lt;- as.data.frame(list_of_draws$centre[1:n_draws,])\rcolnames(centrevals) \u0026lt;- mutants\rwidthvals \u0026lt;- as.data.frame(list_of_draws$width[1:n_draws,])\rcolnames(widthvals) \u0026lt;- mutants\rheightvals \u0026lt;- as.data.frame(list_of_draws$height[1:n_draws,])\rcolnames(heightvals) \u0026lt;- mutants\rconc_detailed \u0026lt;- seq(60,80,0.1)\r#Create a 3D array of draw-mutant-concentration, filled with the probability of success.\rcurves \u0026lt;- array(NA, dim=c(n_draws, n_mutants, length(conc_detailed)), dimnames = list(draws, mutants, conc_detailed))\rfor (i in 1:n_draws){\rfor (j in 1:n_mutants){\rfor (k in 1:length(conc_detailed)){\rcurves[i,j,k] \u0026lt;- gaussian(height = heightvals[i,j], centre = centrevals[i,j], width = widthvals[i,j], x = conc_detailed[k]) }\r}\r}\r#Turn the array into a data frame in tidy format - the fastest way is to use the melt function from reshape2 (unfortunately tidyr\u0026#39;s \u0026#39;gather\u0026#39; does not work on arrays)\rlibrary(reshape2)\rcurves_c \u0026lt;- melt(curves)\rdetach(package:reshape2)\rcolnames(curves_c) \u0026lt;- c(\u0026quot;draw\u0026quot;, \u0026quot;mutant\u0026quot;, \u0026quot;conc\u0026quot;, \u0026quot;prob\u0026quot;)\r Here are some functions to plot the posterior draws.\n# A function from stack exchange by akrun that gives comma separated lists, with an \u0026#39;and\u0026#39; before the final item.\rfPaste \u0026lt;- function(vec) sub(\u0026quot;,\\\\s+([^,]+)$\u0026quot;, \u0026quot; and \\\\1\u0026quot;, toString(vec)) # A function to plot some draws in separate facets\rcompare_mutants_plot \u0026lt;- function(mutantlist, draws_to_plot=12, centre_draws = centrevals, curvedata = curves_c){\rcentrepositions \u0026lt;- centre_draws[1:draws_to_plot,] %\u0026gt;%\rmutate(draw = draws[1:draws_to_plot]) %\u0026gt;%\rgather(mutant, centre,-draw)\routplot \u0026lt;- curvedata %\u0026gt;%\rfilter(mutant %in% mutantlist) %\u0026gt;%\rfilter(draw %in% draws[1:draws_to_plot]) %\u0026gt;%\rggplot()+\raes(x = conc, y = prob, group = mutant, colour = mutant)+\rgeom_line()+\rgeom_vline(data = centrepositions %\u0026gt;% filter(mutant %in% mutantlist), aes(xintercept = centre, colour = mutant))+\rfacet_wrap(~draw)+\rscale_colour_manual(values = c(\u0026quot;blue\u0026quot;, \u0026quot;darkorange\u0026quot;))+\rlabs(title = paste0(\u0026quot;Fitted curves for mutants \u0026quot;, fPaste(mutantlist), \u0026quot;: \u0026quot;, draws_to_plot, \u0026quot; posterior draws\u0026quot;),\rx = \u0026quot;Concentration\u0026quot;, y = \u0026quot;Probability of success\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\rreturn(outplot)\r}\r# A function to overlay many draws\roverlay_mutants_plot \u0026lt;- function(mutantlist, draws_to_plot=200, curvedata = curves_c){\routplot \u0026lt;- curvedata %\u0026gt;%\rfilter(mutant %in% mutantlist) %\u0026gt;%\rggplot()+\raes(x = conc, y = prob, group = interaction(mutant, draw), colour = mutant)+\rgeom_line(alpha = 0.3)+\rscale_colour_manual(values = c(\u0026quot;blue\u0026quot;, \u0026quot;darkorange\u0026quot;))+\rlabs(title = paste0(\u0026quot;Fitted curves for mutants \u0026quot;, fPaste(mutantlist), \u0026quot;: \u0026quot;, draws_to_plot, \u0026quot; posterior draws\u0026quot;), x = \u0026quot;Concentration\u0026quot;, y = \u0026quot;Probability of success\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\rreturn(outplot)\r}\r# A function to plot the distribution of differences between centre values\rdifference_plot \u0026lt;- function(first_mutant = \u0026quot;A\u0026quot;, second_mutant = \u0026quot;C\u0026quot;, parameter = \u0026quot;centre\u0026quot;, drawlist = list_of_draws){\rfirst = match(first_mutant, mutants)\rsecond = match(second_mutant, mutants)\rfirst_mutant_draws \u0026lt;- drawlist[[parameter]][,first]\rsecond_mutant_draws \u0026lt;- drawlist[[parameter]][,second]\routplot \u0026lt;- data.frame(\u0026quot;difference\u0026quot; = first_mutant_draws - second_mutant_draws) %\u0026gt;%\rggplot()+\raes(x = difference)+\rstat_density(geom = \u0026quot;line\u0026quot;, colour = \u0026quot;blue\u0026quot;)+\rlabs(title = paste0(\u0026quot;Distribution of differences of mutants \u0026quot;, first_mutant, \u0026quot; and \u0026quot;, second_mutant, \u0026quot; \u0026quot;, parameter, \u0026quot; values: \u0026quot;, post_warmup*n_chains, \u0026quot; posterior draws\u0026quot;), x = \u0026quot;Difference\u0026quot;)\rreturn(outplot)\r}\r Mutants A and C\rNotably, from the plot above we can see that mutants A and C have partially overlapping distributions. Does this mean our model is implying that A and C could possibly share the same (or a very very close) value for centre? Not necessarily - to be able to make a statement about this we need to make comparisons within draws only. Let’s get a quick idea of how these look:\ncompare_mutants_plot(c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;), 12)\rBut that was just a few draws, here are many more:\noverlay_mutants_plot(c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;))\rBut this is still only a subset of the draws. We have done thousands more post-warmup draws, let’s compare across all of them. Mutant A appears to have a slightly higher peak centre value. In what fraction of draws is the peak centre of mutant A at a higher concentration than the peak centre of mutant C?\ncentre_a.all \u0026lt;- list_of_draws$centre[,1]\rcentre_c.all \u0026lt;- list_of_draws$centre[,3]\rsum(centre_a.all \u0026gt; centre_c.all)/length(centre_a.all)\r## [1] 0.97125\r97.1% of them. So, while we can see that they have very close centre values, mutant A probably has a greater centre value than mutant C. Indeed, we know that the true centre value for mutant A is 1.0 concentration units greater than that of C. We can go one better still, and plot the distribution of their differences in all draws.\ndifference_plot(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;)\rThe median difference is 0.9, a slight underestimate. Looking at the experimental data, there is one concentration point for mutant A to the left of its peak centre value at which an unusually high number of trials were successful, given the underlying probability. This would have the effect of dragging the estimated peak centre to the left. So we are within a regime where we get pretty good estimates, but are of course subject to stochasticity and a bit more data would help. The important point here is that we now have in hand a probability distribution for the difference between these values, rather than a single maximum likelihood estimation, which given the stochastic relationship between paramaters and observations is vastly preferable.\n \rMutants E and F\rWhat about mutants E and F? The distributions for their centre values are entirely overlapping, and we know that their true centre values are identical.\ncompare_mutants_plot(c(\u0026quot;E\u0026quot;, \u0026quot;F\u0026quot;), 12)\roverlay_mutants_plot(c(\u0026quot;E\u0026quot;, \u0026quot;F\u0026quot;))\rIn what fraction of draws is the peak centre of mutant F at a lower concentration than the peak centre of mutant E?\ncentre_e.all \u0026lt;- list_of_draws$centre[,5]\rcentre_f.all \u0026lt;- list_of_draws$centre[,6]\rsum(centre_f.all \u0026lt; centre_e.all)/length(centre_f.all)\r## [1] 0.439125\rMutant F, which is has a true peak centre identical to mutant E was lower than mutant E in approximately half of the draws, so we would not say that there is evidence they differ.\ndifference_plot(\u0026quot;E\u0026quot;, \u0026quot;F\u0026quot;)\r So, the numbers are reasonable and the model is looking good, it did a good job with the simulated data. The next step would be to fit it to the actual experimental data.\n  \r\r","date":1535328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535328000,"objectID":"6e2269ee256c719048cd62dc58b91a04","permalink":"/post/post_1/","publishdate":"2018-08-27T00:00:00Z","relpermalink":"/post/post_1/","section":"post","summary":"In this post, I simulate an example experiment in which there is a non-linear relationship with underlying parameters that vary by group membership, before fitting the data using Bayesian regression in Stan. In this example, inspired by a real-world problem (with details changed to protect future Nobel prize winners, so try not to read too much into it), several mutant strains of a species of bacteria have been exposed to a range of concentrations of Chemical X.","tags":["Stan","regression"],"title":"Simulation and Modelling in R and Stan","type":"post"},{"authors":["Flynn R. Hill","Antoine M. van Oijen","Karl E. Duderstadt"],"categories":null,"content":"","date":1514725200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514725200,"objectID":"3db086e4c4c0a7058072e871b1b23b44","permalink":"/publication/change-points/","publishdate":"2018-01-01T00:00:00+11:00","relpermalink":"/publication/change-points/","section":"publication","summary":"Single-molecule approaches present a powerful way to obtain detailed kinetic information at the molecular level. However, the identification of small rate changes is often hindered by the considerable noise present in such single-molecule kinetic data. We present a general method to detect such kinetic change points in trajectories of motion of processive single molecules having Gaussian noise, with a minimum number of parameters and without the need of an assumed kinetic model beyond piece-wise linearity of motion. Kinetic change points are detected using a likelihood ratio test in which the probability of no change is compared to the probability of a change occurring, given the experimental noise. A predetermined confidence interval minimizes the occurrence of false detections. Applying the method recursively to all sub-regions of a single molecule trajectory ensures that all kinetic change points are located. The algorithm presented allows rigorous and quantitative determination of kinetic change points in noisy single molecule observations without the need for filtering or binning, which reduce temporal resolution and obscure dynamics. The statistical framework for the approach and implementation details are discussed. The detection power of the algorithm is assessed using simulations with both single kinetic changes and multiple kinetic changes that typically arise in observations of single-molecule DNA-replication reactions. Implementations of the algorithm are provided in ImageJ plugin format written in Java and in the Julia language for numeric computing, with accompanying Jupyter Notebooks to allow reproduction of the analysis presented here.","tags":[],"title":"Detection of kinetic change points in piece-wise linear single molecule motion","type":"publication"}]